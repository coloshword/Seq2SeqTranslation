{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq model for English to Chinese translation (baseline model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing, loading, and cleaning dataset\n",
    "\n",
    "- going to use datasets from: https://www.kaggle.com/datasets/qianhuan/translation?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5161434\n",
      "{'english': 'For greater sharpness, but with a slight increase in graininess, you can use a 1:1 dilution of this developer.', 'chinese': '为了更好的锐度，但是附带的会多一些颗粒度，可以使用这个显影剂的1：1稀释液。'}\n"
     ]
    }
   ],
   "source": [
    "train_set_path = \"dataset/translation2019zh_train.json\"\n",
    "\n",
    "train_set = []\n",
    "with open(train_set_path) as f:\n",
    "    for line in f:\n",
    "        train_set.append(json.loads(line))\n",
    "\n",
    "print(len(train_set))\n",
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to lower the size of this dataset, for testing purposes.\n",
    "- right now 5.1M sentences\n",
    "- let's make it 10,000 sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 'Introduce:Kitty (Pamila Jideli is acted the role of) it is a cameraman, in fiance De Kesi (conspicuous of Jeff · law is acted the role of) magazine company works.', 'chinese': '简介：凯蒂（帕米拉·吉德利饰）是一名摄影师，在未婚夫德克斯（杰夫·法赫饰）的杂志社工作。'}\n"
     ]
    }
   ],
   "source": [
    "# get 10,000 random indices \n",
    "sampled_indices = np.random.choice(len(train_set), 100000)\n",
    "\n",
    "train_subset = [train_set[i] for i in sampled_indices]\n",
    "print(train_subset[0])\n",
    "with open('dataset/train_set_mini.pkl', 'wb') as f:\n",
    "    pickle.dump(train_subset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 'Introduce:Kitty (Pamila Jideli is acted the role of) it is a cameraman, in fiance De Kesi (conspicuous of Jeff · law is acted the role of) magazine company works.', 'chinese': '简介：凯蒂（帕米拉·吉德利饰）是一名摄影师，在未婚夫德克斯（杰夫·法赫饰）的杂志社工作。'}\n"
     ]
    }
   ],
   "source": [
    "with open('dataset/train_set_mini.pkl', 'rb') as f:\n",
    "    train_set_mini = pickle.load(f)\n",
    "\n",
    "print(train_set_mini[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train set made. \n",
    "Now to work on the actual architecture\n",
    "\n",
    "## Preprocessing steps:\n",
    "- we want to maintain vocabulary for english and chinese. \n",
    "    - this is simple, just use a counter() and then limit it to if it appears within 5\n",
    "\n",
    "- In terms of encoding, we want to use sequence input, so a sentence becomes a list [3, 100, 8, 9], where each number corresponds to the index of the word in the dictionary\n",
    "    - we want to do this because LSTMs keep track of the word relationships at the sentence level\n",
    "    - then use nn.Embedding?\n",
    "        - nn.Embedding allows us to create a matrix representing the vocabulary. It allows us to train some nuance into the words, where instead of typical BoW where each word is just an index, each word is now a vector, which allows us to train some meaning into the word\n",
    "        - the embedding is a matrix size (vocab length, dim). Length vocab length because each row in the matrix corresponds to a word in the vocab, ( row of index = index of word in vocab :) )\n",
    "        \n",
    "\n",
    "# now to work on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words pre-clean 1912997\n",
      "Words post-clean 1903544\n",
      "20384\n",
      "3271271\n",
      "5094\n"
     ]
    }
   ],
   "source": [
    "## helper functions \n",
    "def remove_punctuation(text):\n",
    "    '''\n",
    "    Get rid of all punctuation from string text\n",
    "    '''\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def get_words_from_sentence(s):\n",
    "    '''\n",
    "    Gets words from sentence \n",
    "    '''\n",
    "    return s.split(' ')\n",
    "\n",
    "def clean_en_pair(pair):\n",
    "    '''\n",
    "    Cleans the english from the pair \n",
    "    '''\n",
    "    return get_words_from_sentence(remove_punctuation(pair['english']).lower())\n",
    "\n",
    "def get_en_vocab(train_set):\n",
    "    '''\n",
    "    get_en_dict:\n",
    "        Gets an english vocab from train_set as a dict \n",
    "    '''\n",
    "    # get only the english sentences, list of strings \n",
    "    en_sentences = [clean_en_pair(pair) for pair in train_set]\n",
    "    en_sentences_flattened = [word for sentence in en_sentences for word in sentence]\n",
    "    print(f\"Words pre-clean {len(en_sentences_flattened)}\")\n",
    "    en_sentences_flattened = [word for word in en_sentences_flattened if word != '']\n",
    "    print(f\"Words post-clean {len(en_sentences_flattened)}\")\n",
    "    \n",
    "    word_counts = Counter(en_sentences_flattened)\n",
    "    # with word counts, now we limit the vocabulary to words that happen at least 5 times\n",
    "    en_vocab = {}\n",
    "    # {word: index}\n",
    "    idx = 0\n",
    "    for word in [\"<SOS>\", \"<EOS>\", \"<UNK>\"]:\n",
    "        en_vocab[word] = idx \n",
    "        idx += 1\n",
    "    for word, occurrences in word_counts.items():\n",
    "        if occurrences >= 5:\n",
    "            en_vocab[word] = idx \n",
    "            idx += 1\n",
    "    return en_vocab\n",
    "\n",
    "def remove_zh_punctuation(text):\n",
    "    cleaned = re.sub(r'[，。！？【】（）《》“”‘’、]', '', text)\n",
    "    cleaned = re.sub(r'\\s+', '', cleaned)\n",
    "    return cleaned\n",
    "\n",
    "def get_zh_vocab(train_set):\n",
    "    '''\n",
    "    get_zh_vocab:\n",
    "        Gets an zh vocab from train_set as a dict \n",
    "    '''\n",
    "    zh_sentences = [list(remove_zh_punctuation(pair['chinese'])) for pair in train_set]\n",
    "    zh_sentences_flattened = [word for sentence in zh_sentences for word in sentence]\n",
    "    print(len(zh_sentences_flattened))\n",
    "\n",
    "    word_counts = Counter(zh_sentences_flattened)\n",
    "    zh_vocab = {}\n",
    "\n",
    "    idx = 0 \n",
    "    for word in [\"<SOS>\", \"<EOS>\", \"<UNK>\"]:\n",
    "        zh_vocab[word] = idx \n",
    "        idx += 1 \n",
    "    for word, occurrences in word_counts.items():\n",
    "        if occurrences >= 2: \n",
    "            zh_vocab[word] = idx \n",
    "            idx += 1 \n",
    "    return zh_vocab\n",
    "\n",
    "en_vocab = get_en_vocab(train_set_mini)\n",
    "print(len(en_vocab))\n",
    "\n",
    "zh_vocab = get_zh_vocab(train_set_mini)\n",
    "print(len(zh_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab/en_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(en_vocab, f)\n",
    "\n",
    "with open('vocab/zh_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(zh_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab/en_vocab.pkl', 'rb') as f:\n",
    "    en_vocab = pickle.load(f)\n",
    "\n",
    "with open('vocab/zh_vocab.pkl', 'rb') as f:\n",
    "    zh_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture building\n",
    "- 2 LSTM's are the backbone\n",
    "- also build a higher level Seq2Seq model as abstraction of the entire model \n",
    "- nn.Embedding() as a variable for both Encoder and Decoder \n",
    "    - use vocab_size as row length, by the embedding dim as the column length\n",
    "- Encoder will be english, decoder will be chinese "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.LSTM\n",
    "\n",
    "- sequence models are central to NLPl they are models where there is some sort of dependence through teime between inputs. \n",
    "- a recurrent neural network is a network that maintains some kind of state.\n",
    "- for example its output could be used as part of the next input, so that information c an propagate along as the network passes over the sequence.\n",
    "- In the case of an LSTM, for each element in the sequence, there is a corresponding hidden state ht, which in principle contains infromation from arbitrary points earlier in the sequence. \n",
    "- we can use the hidden state to predict words in a language model, pos, and a myriad of oether things.\n",
    "\n",
    "LSTMs in pytorch:\n",
    "- pytorch LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The first axis is the sequence itself, the second indixes instances in the mini batch, and the third indexes elements of the input. \n",
    "- ignore mini batching, we will always just have 1 dimension on th second axis.\n",
    "- If we want to run the sequence model over the sentence \"The cow jumped\" our input should look like:\n",
    "\n",
    "[\n",
    "    q (the)\n",
    "    q (cow)\n",
    "    q (jumped)\n",
    "]\n",
    "Except remember there is an additional 2nd dimension with size 1, (this dimension )\n",
    "\n",
    "Initializing an LSTM:\n",
    "```python\n",
    "lstm = nn.LSTM(3, 3) #input dim is 3, output dim is 3 \n",
    "```\n",
    "\n",
    "input_size = 3: this means each input vector at a time step is of length 3. All inputs must have 3 columns, (n x 3). \n",
    "- each sequence = a list of input vectors (one per timestemp)\n",
    "- each input vector = size input_size \n",
    "\n",
    "input_size = 3, then your input tensor shape for 1 batch would be:\n",
    "    (seq_len, batch_size, 3)\n",
    "\n",
    "- What does this mean for the embedding layer?\n",
    "    - it must also be dimension (vocab_size, 3), since each token is mapped to a vecotr using the embedding. This embedding becomes the input at each timestep for the LSTM, and the LSTM accepts vector of dimensions (3).\n",
    "\n",
    "- using nn.LSTM example:\n",
    "    \n",
    "```python\n",
    "self.lstm = nn.LSTM(embedding_dim, hidden_dim)  #(embedding_dim) is the dimension of the embedding dim, and then hidden_dim is also essentially a hyperparameter, it's the dimension of the hidden state \n",
    "```\n",
    "\n",
    "**For most LSTM applications we will need a linear layer to learn the mapping from hidden state to the tag space. but in the case of the encoder, you don't need it! Since the linear layer is essentially the classifier layer, that learns to \n",
    "interpret the hidden layer.**\n",
    "\n",
    "\n",
    "### forward:\n",
    "- in forward we will have\n",
    "```\n",
    "lstm_out, _ = self.LSTM(embeds.view(len(sentence), 1, -1))\n",
    "```\n",
    "- what exactly is going on here? Well, basically embeds is a tensor of dimensions (number of tokens in sentence, embedding dim) (**since rememember that the embedding layer takes each word index, and pulls up the corresponding row from the vocabulary)\n",
    "- we want to reshape it to (sequence length, batch_size, input_size)\n",
    "    - reshaping is: sequence length( the length of the sentence )\n",
    "    - batch size is 1, since its one sentence at a time \n",
    "    - input_size = -1: it assumes that the length will be embedding_dim \n",
    "- **embeds.view**: is a way to reshape in LSTM\n",
    "\n",
    "- forward in the encoder should only return the hidden state and cell state, since its what matters \n",
    "\n",
    "\n",
    "## backpropagation\n",
    "- pytorch under the hood tracks operations on tensors with require_grad = True. All nn.modules like nn.Linear and nn.LSTM already register their parameters with requires_grad=True, so as long as its connected correctly in the forward pass, pytorch will handle the gradients during backprop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder: English layer \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim) # initialize an LSTM, with embedding_dim, and hidden_dim hyperparameters \n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeddings(sentence)  # remember that sentence has to the in [word_index0, word_index1, word_index2] form\n",
    "        _, (h_n, c_n) = self.LSTM(embeds.view(len(sentence), 1, -1)) # one timestep at a tiem \n",
    "        return h_n, c_n\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[261, 256, 4173]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test example pass through the encoder \n",
    "encoder = Encoder(embedding_dim=3, vocab_size= len(en_vocab), hidden_dim=5)\n",
    "# now remember that for forward we pass a sentence as the list of words mapped to the indices they show up in the vocab, such as [45, 18, 28]\n",
    "sentence = \"I love bread.\"\n",
    "\n",
    "input_words = get_words_from_sentence(remove_punctuation(sentence).lower())\n",
    "# now map the inputs to the vocab \n",
    "input_indices = [en_vocab[word] for word in input_words] \n",
    "# now that I think about it, we probably want a function that does this, so that we don't get hit with a KeyError and actually use our <unk> token lul\n",
    "input_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0408, -0.1320, -0.1105, -0.1398,  0.1021]]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " tensor([[[ 0.0731, -0.3006, -0.2242, -0.3209,  0.1431]]],\n",
       "        grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with the input_indices, we can now throw it through the encoder?\n",
    "# oh wait yopu need tensor first lul\n",
    "input_indices_tensor = torch.tensor(input_indices, dtype=torch.long)\n",
    "output = encoder.forward(input_indices_tensor)\n",
    "output # this makes sense, we have both hidden and cell states :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "- so the decoder is another LSTM, taking as input \"a large fixed-dimensional vector representation\", and then use another LSTM to extract the output sequence from that vector \n",
    "- we can just pass in h_n and c_n in the decoder LSTM as parameter for this! \n",
    "- and then for forward we just run the linear layer and then run the log_softmax to get the logits?\n",
    "\n",
    "- What about the Embedding layer?\n",
    "    - we also need an embedding layer ( used in both training and inference )\n",
    "    - training phase we have \"ground-truth\" tokens, we need the \"ground-truth\" tokens we need the embedding layer to make them tensors and to feed each token through the decoder\n",
    "\n",
    "### Teacher forcing and backpropagation\n",
    "- At a time step t, the input is the actual target sequence from t - 1 !. This makes sense, we give it the \"correct\" input from the time before, and have it try to predict the input now.  the t-1 token is called the \"ground truth\" token, is passed through the embedding layer specifically trained for the target language vocabulary\n",
    "- the output is the predicted timestep t token, and what you use to compare as loss is the actual t token. \n",
    "\n",
    "**forward step in the decoder**\n",
    "- when making a prediction, you either use the correct previous token (teacher forcing during training), or you use the previous prediction (inference)\n",
    "- during prediction we will use nn.LSTM. You want to pass the token at t-1's tensor, AND the previous c_n and h_n from the decoder! it's the recurrent aspect of the RNN\n",
    "\n",
    "- we also do feed in the encoder h_n, and c_n, in the first step :).\n",
    "- don't need to worry about the rest, because it already does it implicitly by nn.LSTM\n",
    "\n",
    "- also to enforce teacher forcing, we have to do one time step at a time, instead of all at once \n",
    "    - so instead, we go through every single input one at time \n",
    "\n",
    "- in the no teacher YOU USE THE PREVIOUS PREDICTION OF THE TIME(after it is )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurations \n",
    "MAX_RESPONSE_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_dim, device):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.device = device\n",
    "\n",
    "    def word_to_tensor(self, word):\n",
    "        '''\n",
    "        takes a single wrod and gets the corresponding tensor\n",
    "        '''\n",
    "        word_lst = get_words_from_sentence(remove_zh_punctuation(word))\n",
    "        indices = [zh_vocab[word] for word in word_lst]\n",
    "        # get tensor \n",
    "        return torch.tensor(indices, dtype=torch.long).to(self.device)\n",
    "    \n",
    "    def forward(self, hidden, sentence=None):\n",
    "        '''\n",
    "        does the forward propagation. If sentence is provided, then we do teacher-forcing. Else we assume it is inference  \n",
    "            Params:\n",
    "                hidden: the hidden state passed from the previous \n",
    "                sentence: a sentence to be used for teacher-forcing, as a tensor \n",
    "                Make sure the teacher-forcing sentence is sliced to not include the last token [:-1]\n",
    "        '''\n",
    "        if sentence is not None:\n",
    "            # use teacher forcing \n",
    "            embeds = self.embeddings(sentence)\n",
    "            embeds_reshaped = embeds.view(len(embeds), 1, -1)\n",
    "            out, hidden = self.LSTM(embeds_reshaped, hidden)\n",
    "            logits = self.linear(out)\n",
    "            return logits\n",
    "        else:\n",
    "            # inference can stay the same (we want one at a time to check if the out put is eos)\n",
    "            # '<SOS>' token \n",
    "            # just generate some tokens, starting from the <sos> token\n",
    "            start_token = self.word_to_tensor('<SOS>')\n",
    "            # run through embedding layer\n",
    "            prev_char = start_token\n",
    "            outputs = []\n",
    "            for i in range(MAX_RESPONSE_LENGTH):\n",
    "                embeds = self.embeddings(prev_char).to(self.device)\n",
    "                out, hidden = self.LSTM(embeds.view(1, 1, -1), hidden)\n",
    "                logits = self.linear(out)\n",
    "                outputs.append(logits)\n",
    "                pred_idx = torch.argmax(logits, dim=2).item()\n",
    "                prev_char = torch.tensor(pred_idx, dtype=torch.long, device=self.device)\n",
    "            return torch.cat(outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions to take a sentence and turn it into a tensor, adding <sos> and <eos>\n",
    "def sequence_to_tensor_en(sequence):\n",
    "    '''\n",
    "    takes sequence and converts to tensor \n",
    "    '''\n",
    "    # add \"<SOS> and <EOS>\"\n",
    "    words = get_words_from_sentence(\"<SOS> \" + remove_punctuation(sequence).lower() + \" <EOS>\")\n",
    "    \n",
    "    # convert to indices, reverting to <UNK> token\n",
    "    word_indices = [ en_vocab[word] if word in en_vocab else en_vocab[\"<UNK>\"] for word in words ]\n",
    "    return torch.tensor(word_indices, dtype=torch.long)\n",
    "    \n",
    "\n",
    "def sequence_to_tensor_zh(sequence):\n",
    "    '''\n",
    "    takes sequence and converts to chinese tensor \n",
    "    '''\n",
    "    words = ([\"<SOS>\"] + list(remove_zh_punctuation(sequence)))\n",
    "    words.append(\"<EOS>\")\n",
    "    \n",
    "    word_indices = [ zh_vocab[word] if word in zh_vocab else zh_vocab[\"<UNK>\"] for word in words ]\n",
    "    return torch.tensor(word_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a full run through both the Encoder and the decoder \n",
    "\n",
    "encoder = Encoder(embedding_dim=3, vocab_size=len(en_vocab), hidden_dim=5)\n",
    "en_sentence = \"I love bread.\"\n",
    "zh_sentence = \"我爱面包\"\n",
    "\n",
    "\n",
    "h_n, c_n = encoder.forward(sequence_to_tensor_en(en_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train  100000\n",
      "Length of zh dictionary  5094\n",
      "Length of english dictionary  20384\n"
     ]
    }
   ],
   "source": [
    "## new 100,000 len\n",
    "print(\"Length of train \", len(train_set_mini))\n",
    "print(\"Length of zh dictionary \", len(zh_vocab))\n",
    "print(\"Length of english dictionary \", len(en_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zh_tensor_outputs_to_sentence(output_tensor):\n",
    "    s = ''\n",
    "    zh_vocab_lst = list(zh_vocab.keys())\n",
    "    for word_tensor in output_tensor:\n",
    "        pred_idx = torch.argmax(word_tensor, dim=-1).item()\n",
    "        s += zh_vocab_lst[pred_idx]\n",
    "    return s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does mps save time?\n",
    "琥ä纷随哗良钮眼黎s\n",
    "Number of trains 1000\n",
    "Loss 6.893100890159607\n",
    "time for 1000 : mps: 13.119836807250977\n",
    "\n",
    "CPU:\n",
    "Number of trains 1000\n",
    "Loss 6.896958403587341\n",
    "time for 1000 : cpu: 23.142292022705078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "急蚱蚱蚱院車橼蟋息馏\n",
      "Number of trains 1000\n",
      "Loss 6.904454550743103\n",
      "time for 1000 : mps: 12.955302000045776\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m encoder\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     50\u001b[0m decoder\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 52\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set_mini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 32\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, training_data, encoder, decoder, device, lr)\u001b[0m\n\u001b[1;32m     29\u001b[0m en_tensor \u001b[38;5;241m=\u001b[39m sequence_to_tensor_en(english)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m zh_tensor \u001b[38;5;241m=\u001b[39m sequence_to_tensor_zh(zh)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 32\u001b[0m h_n, c_n \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43men_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m target \u001b[38;5;241m=\u001b[39m zh_tensor[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     34\u001b[0m predicted \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mforward((h_n, c_n), zh_tensor[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, sentence)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence):\n\u001b[0;32m---> 10\u001b[0m     embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# remember that sentence has to the in [word_index0, word_index1, word_index2] form\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     _, (h_n, c_n) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLSTM(embeds\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mlen\u001b[39m(sentence), \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;66;03m# one timestep at a tiem \u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m h_n, c_n\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train(num_epochs, training_data, encoder, decoder, device, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()), lr=lr\n",
    ")\n",
    "    count = 0\n",
    "    total_loss = 0\n",
    "    # see random prediction\n",
    "    predict_en_sequence = \"I love bread\" \n",
    "    predict_en_tensor = sequence_to_tensor_en(predict_en_sequence).to(device)\n",
    "    predict_hidden = encoder(predict_en_tensor)\n",
    "    print(zh_tensor_outputs_to_sentence(decoder.forward(predict_hidden)))\n",
    "    start_time = time.time()\n",
    "    for i in range(num_epochs):\n",
    "        for pair in training_data:\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(f\"Number of trains {count}\")\n",
    "                # print the loss \n",
    "                print(f\"Loss {total_loss / 1000}\")\n",
    "                total_loss = 0\n",
    "            english = pair['english']\n",
    "            zh = pair['chinese']\n",
    "            en_tensor = sequence_to_tensor_en(english)\n",
    "            zh_tensor = sequence_to_tensor_zh(zh)\n",
    "            # pass to device \n",
    "            en_tensor = sequence_to_tensor_en(english).to(device)\n",
    "            zh_tensor = sequence_to_tensor_zh(zh).to(device)\n",
    "\n",
    "            h_n, c_n = encoder.forward(en_tensor)\n",
    "            target = zh_tensor[1:]\n",
    "            predicted = decoder.forward((h_n, c_n), zh_tensor[:-1])\n",
    "            \n",
    "            loss = nn.functional.cross_entropy(torch.squeeze(predicted), target)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        predict_hidden = encoder(predict_en_tensor)\n",
    "        print(zh_tensor_outputs_to_sentence(decoder.forward(predict_hidden)))\n",
    "    print(f\"Total training time: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "encoder = Encoder(embedding_dim=32, vocab_size=len(en_vocab), hidden_dim=128)\n",
    "decoder = Decoder(embedding_dim=32, vocab_size=len(zh_vocab), hidden_dim=128, device=device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "train(2, train_set_mini, encoder, decoder, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save encoder and decoder \n",
    "torch.save(encoder.state_dict(), './trained_models/baseline_encoder.pth')\n",
    "torch.save(decoder.state_dict(), './trained_models/baseline_decoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Decoder(\n",
       "  (embeddings): Embedding(5125, 32)\n",
       "  (LSTM): LSTM(32, 128)\n",
       "  (linear): Linear(in_features=128, out_features=5125, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load('./trained_models/baseline_encoder.pth'))\n",
    "decoder.load_state_dict(torch.load('./trained_models/baseline_decoder.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我想要做一个一个<EOS><EOS>\n"
     ]
    }
   ],
   "source": [
    "def translate_en_to_zh(encoder, decoder, sequence):\n",
    "        hidden = encoder.forward(sequence_to_tensor_en(sequence))\n",
    "        predicted_tensor, target = decoder.forward(hidden)\n",
    "        print(decoder.tensor_outputs_to_sentence(predicted_tensor))\n",
    "\n",
    "translate_en_to_zh(encoder, decoder, \"I need to eat a cat\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
