{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq model for English to Chinese translation (baseline model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing, loading, and cleaning dataset\n",
    "\n",
    "- going to use datasets from: https://www.kaggle.com/datasets/qianhuan/translation?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5161434\n",
      "{'english': 'For greater sharpness, but with a slight increase in graininess, you can use a 1:1 dilution of this developer.', 'chinese': '为了更好的锐度，但是附带的会多一些颗粒度，可以使用这个显影剂的1：1稀释液。'}\n"
     ]
    }
   ],
   "source": [
    "train_set_path = \"dataset/translation2019zh_train.json\"\n",
    "\n",
    "train_set = []\n",
    "with open(train_set_path) as f:\n",
    "    for line in f:\n",
    "        train_set.append(json.loads(line))\n",
    "\n",
    "print(len(train_set))\n",
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to lower the size of this dataset, for testing purposes.\n",
    "- right now 5.1M sentences\n",
    "- let's make it 10,000 sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 'The amphibian & reptile hall, the marine animal hall, the tropic monkey hall, the bird garden are to be built on the flat ground, and the Garden will welcome Chinese and foreign guests with new looks.', 'chinese': '两栖——爬行动物馆、海兽馆、热带猴馆、鸟园等新型建筑将平地而起，以新的容貌迎接中外宾客。'}\n"
     ]
    }
   ],
   "source": [
    "# get 10,000 random indices \n",
    "sampled_indices = np.random.choice(len(train_set), 10000)\n",
    "\n",
    "train_subset = [train_set[i] for i in sampled_indices]\n",
    "print(train_subset[0])\n",
    "with open('dataset/train_set_mini.pkl', 'wb') as f:\n",
    "    pickle.dump(train_subset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 'The amphibian & reptile hall, the marine animal hall, the tropic monkey hall, the bird garden are to be built on the flat ground, and the Garden will welcome Chinese and foreign guests with new looks.', 'chinese': '两栖——爬行动物馆、海兽馆、热带猴馆、鸟园等新型建筑将平地而起，以新的容貌迎接中外宾客。'}\n"
     ]
    }
   ],
   "source": [
    "with open('dataset/train_set_mini.pkl', 'rb') as f:\n",
    "    train_set_mini = pickle.load(f)\n",
    "\n",
    "print(train_set_mini[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train set made. \n",
    "Now to work on the actual architecture\n",
    "\n",
    "## Preprocessing steps:\n",
    "- we want to maintain vocabulary for english and chinese. \n",
    "    - this is simple, just use a counter() and then limit it to if it appears within 5\n",
    "\n",
    "- In terms of encoding, we want to use sequence input, so a sentence becomes a list [3, 100, 8, 9], where each number corresponds to the index of the word in the dictionary\n",
    "    - we want to do this because LSTMs keep track of the word relationships at the sentence level\n",
    "    - then use nn.Embedding?\n",
    "        - nn.Embedding allows us to create a matrix representing the vocabulary. It allows us to train some nuance into the words, where instead of typical BoW where each word is just an index, each word is now a vector, which allows us to train some meaning into the word\n",
    "        - the embedding is a matrix size (vocab length, dim). Length vocab length because each row in the matrix corresponds to a word in the vocab, ( row of index = index of word in vocab :) )\n",
    "        \n",
    "\n",
    "# now to work on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_l\n\u001b[1;32m     12\u001b[0m train_set_mini[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_en_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set_mini\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[19], line 7\u001b[0m, in \u001b[0;36mget_en_dict\u001b[0;34m(train_set)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mget_en_dict:\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m    Gets an english dict from train_set \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m      6\u001b[0m new_l \u001b[38;5;241m=\u001b[39m [train_set[\u001b[38;5;241m0\u001b[39m], train_set[\u001b[38;5;241m1\u001b[39m]]\n\u001b[0;32m----> 7\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mCounter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_l\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(c)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_l\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/collections/__init__.py:607\u001b[0m, in \u001b[0;36mCounter.__init__\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''Create a new, empty Counter object.  And if given, count elements\u001b[39;00m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;124;03mfrom an input iterable.  Or, initialize the count from another mapping\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;124;03mof elements to their counts.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m \n\u001b[1;32m    605\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 607\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/collections/__init__.py:698\u001b[0m, in \u001b[0;36mCounter.update\u001b[0;34m(self, iterable, **kwds)\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mupdate(iterable)\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 698\u001b[0m         \u001b[43m_count_elements\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "def get_en_dict(train_set):\n",
    "    '''\n",
    "    get_en_dict:\n",
    "        Gets an english dict from train_set \n",
    "    '''\n",
    "    new_l = [train_set[0], train_set[1]]\n",
    "    c = Counter(new_l)\n",
    "    print(c)\n",
    "    return new_l\n",
    "    \n",
    "\n",
    "train_set_mini[0]['english']\n",
    "print(get_en_dict(train_set_mini))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
