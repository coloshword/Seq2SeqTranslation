{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq model for English to Chinese translation (baseline model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing, loading, and cleaning dataset\n",
    "\n",
    "- going to use datasets from: https://www.kaggle.com/datasets/qianhuan/translation?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5161434\n",
      "{'english': 'For greater sharpness, but with a slight increase in graininess, you can use a 1:1 dilution of this developer.', 'chinese': '为了更好的锐度，但是附带的会多一些颗粒度，可以使用这个显影剂的1：1稀释液。'}\n"
     ]
    }
   ],
   "source": [
    "train_set_path = \"dataset/translation2019zh_train.json\"\n",
    "\n",
    "train_set = []\n",
    "with open(train_set_path) as f:\n",
    "    for line in f:\n",
    "        train_set.append(json.loads(line))\n",
    "\n",
    "print(len(train_set))\n",
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to lower the size of this dataset, for testing purposes.\n",
    "- right now 5.1M sentences\n",
    "- let's make it 10,000 sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 'The present invention provides a high-pressure high-strength gypsum block and its automatic water-filtering continuously-forming machine used in the course of press-forming process.', 'chinese': '本发明提供一种高压高强度的石膏砌块及其在压制成型过程中自动滤水的连续成型机，以提高工作效率和改善石膏砌块成型质量和性能。'}\n"
     ]
    }
   ],
   "source": [
    "# get 10,000 random indices \n",
    "sampled_indices = np.random.choice(len(train_set), 10000)\n",
    "\n",
    "train_subset = [train_set[i] for i in sampled_indices]\n",
    "print(train_subset[0])\n",
    "with open('dataset/train_set_mini.pkl', 'wb') as f:\n",
    "    pickle.dump(train_subset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 'The present invention provides a high-pressure high-strength gypsum block and its automatic water-filtering continuously-forming machine used in the course of press-forming process.', 'chinese': '本发明提供一种高压高强度的石膏砌块及其在压制成型过程中自动滤水的连续成型机，以提高工作效率和改善石膏砌块成型质量和性能。'}\n"
     ]
    }
   ],
   "source": [
    "with open('dataset/train_set_mini.pkl', 'rb') as f:\n",
    "    train_set_mini = pickle.load(f)\n",
    "\n",
    "print(train_set_mini[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train set made. \n",
    "Now to work on the actual architecture\n",
    "\n",
    "## Preprocessing steps:\n",
    "- we want to maintain vocabulary for english and chinese. \n",
    "    - this is simple, just use a counter() and then limit it to if it appears within 5\n",
    "\n",
    "- In terms of encoding, we want to use sequence input, so a sentence becomes a list [3, 100, 8, 9], where each number corresponds to the index of the word in the dictionary\n",
    "    - we want to do this because LSTMs keep track of the word relationships at the sentence level\n",
    "    - then use nn.Embedding?\n",
    "        - nn.Embedding allows us to create a matrix representing the vocabulary. It allows us to train some nuance into the words, where instead of typical BoW where each word is just an index, each word is now a vector, which allows us to train some meaning into the word\n",
    "        - the embedding is a matrix size (vocab length, dim). Length vocab length because each row in the matrix corresponds to a word in the vocab, ( row of index = index of word in vocab :) )\n",
    "        \n",
    "\n",
    "# now to work on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions \n",
    "def remove_punctuation(text):\n",
    "    '''\n",
    "    Get rid of all punctuation from string text\n",
    "    '''\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def get_words_from_sentence(s):\n",
    "    '''\n",
    "    Gets words from sentence \n",
    "    '''\n",
    "    return s.split(' ')\n",
    "\n",
    "def clean_en_pair(pair):\n",
    "    '''\n",
    "    Cleans the english from the pair \n",
    "    '''\n",
    "    return get_words_from_sentence(remove_punctuation(pair['english']).lower())\n",
    "\n",
    "def get_en_vocab(train_set):\n",
    "    '''\n",
    "    get_en_dict:\n",
    "        Gets an english vocab from train_set as a dict \n",
    "    '''\n",
    "    # get only the english sentences, list of strings \n",
    "    en_sentences = [clean_en_pair(pair) for pair in train_set]\n",
    "    en_sentences_flattened = [word for sentence in en_sentences for word in sentence]\n",
    "    print(f\"Words pre-clean {len(en_sentences_flattened)}\")\n",
    "    en_sentences_flattened = [word for word in en_sentences_flattened if word != '']\n",
    "    print(f\"Words post-clean {len(en_sentences_flattened)}\")\n",
    "    \n",
    "    word_counts = Counter(en_sentences_flattened)\n",
    "    # with word counts, now we limit the vocabulary to words that happen at least 5 times\n",
    "    en_vocab = {}\n",
    "    # {word: index}\n",
    "    idx = 0\n",
    "    for word in [\"<SOS>\", \"<EOS>\", \"<UNK>\"]:\n",
    "        en_vocab[word] = idx \n",
    "        idx += 1\n",
    "    for word, occurrences in word_counts.items():\n",
    "        if occurrences >= 5:\n",
    "            en_vocab[word] = idx \n",
    "            idx += 1\n",
    "    return en_vocab\n",
    "\n",
    "def remove_zh_punctuation(text):\n",
    "    cleaned = re.sub(r'[，。！？【】（）《》“”‘’、]', '', text)\n",
    "    cleaned = re.sub(r'\\s+', '', cleaned)\n",
    "    return cleaned\n",
    "\n",
    "def get_zh_vocab(train_set):\n",
    "    '''\n",
    "    get_zh_vocab:\n",
    "        Gets an zh vocab from train_set as a dict \n",
    "    '''\n",
    "    zh_sentences = [list(remove_zh_punctuation(pair['chinese'])) for pair in train_set]\n",
    "    zh_sentences_flattened = [word for sentence in zh_sentences for word in sentence]\n",
    "    print(len(zh_sentences_flattened))\n",
    "\n",
    "    word_counts = Counter(zh_sentences_flattened)\n",
    "    zh_vocab = {}\n",
    "\n",
    "    idx = 0 \n",
    "    for word in [\"<SOS>\", \"<EOS>\", \"<UNK>\"]:\n",
    "        zh_vocab[word] = idx \n",
    "        idx += 1 \n",
    "    for word, occurrences in word_counts.items():\n",
    "        if occurrences >= 5: \n",
    "            zh_vocab[word] = idx \n",
    "            idx += 1 \n",
    "    return zh_vocab\n",
    "\n",
    "en_vocab = get_en_vocab(train_set_mini)\n",
    "print(en_vocab)\n",
    "\n",
    "zh_vocab = get_zh_vocab(train_set_mini)\n",
    "print(zh_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab/en_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(en_vocab, f)\n",
    "\n",
    "with open('vocab/zh_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(zh_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab/en_vocab.pkl', 'rb') as f:\n",
    "    en_vocab = pickle.load(f)\n",
    "\n",
    "with open('vocab/zh_vocab.pkl', 'rb') as f:\n",
    "    zh_vocab = pickle.load(f)\n",
    "print(en_vocab)\n",
    "print(zh_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture building\n",
    "- 2 LSTM's are the backbone\n",
    "- also build a higher level Seq2Seq model as abstraction of the entire model \n",
    "- nn.Embedding() as a variable for both Encoder and Decoder \n",
    "    - use vocab_size as row length, by the embedding dim as the column length\n",
    "- Encoder will be english, decoder will be chinese "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.LSTM\n",
    "\n",
    "- sequence models are central to NLPl they are models where there is some sort of dependence through teime between inputs. \n",
    "- a recurrent neural network is a network that maintains some kind of state.\n",
    "- for example its output could be used as part of the next input, so that information c an propagate along as the network passes over the sequence.\n",
    "- In the case of an LSTM, for each element in the sequence, there is a corresponding hidden state ht, which in principle contains infromation from arbitrary points earlier in the sequence. \n",
    "- we can use the hidden state to predict words in a language model, pos, and a myriad of oether things.\n",
    "\n",
    "LSTMs in pytorch:\n",
    "- pytorch LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The first axis is the sequence itself, the second indixes instances in the mini batch, and the third indexes elements of the input. \n",
    "- ignore mini batching, we will always just have 1 dimension on th second axis.\n",
    "- If we want to run the sequence model over the sentence \"The cow jumped\" our input should look like:\n",
    "\n",
    "[\n",
    "    q (the)\n",
    "    q (cow)\n",
    "    q (jumped)\n",
    "]\n",
    "Except remember there is an additional 2nd dimension with size 1, (this dimension )\n",
    "\n",
    "Initializing an LSTM:\n",
    "```python\n",
    "lstm = nn.LSTM(3, 3) #input dim is 3, output dim is 3 \n",
    "```\n",
    "\n",
    "input_size = 3: this means each input vector at a time step is of length 3. All inputs must have 3 columns, (n x 3). \n",
    "- each sequence = a list of input vectors (one per timestemp)\n",
    "- each input vector = size input_size \n",
    "\n",
    "input_size = 3, then your input tensor shape for 1 batch would be:\n",
    "    (seq_len, batch_size, 3)\n",
    "\n",
    "- What does this mean for the embedding layer?\n",
    "    - it must also be dimension (vocab_size, 3), since each token is mapped to a vecotr using the embedding. This embedding becomes the input at each timestep for the LSTM, and the LSTM accepts vector of dimensions (3).\n",
    "\n",
    "- using nn.LSTM example:\n",
    "    \n",
    "```python\n",
    "self.lstm = nn.LSTM(embedding_dim, hidden_dim)  #(embedding_dim) is the dimension of the embedding dim, and then hidden_dim is also essentially a hyperparameter, it's the dimension of the hidden state \n",
    "```\n",
    "\n",
    "**For most LSTM applications we will need a linear layer to learn the mapping from hidden state to the tag space. but in the case of the encoder, you don't need it! Since the linear layer is essentially the classifier layer, that learns to \n",
    "interpret the hidden layer.**\n",
    "\n",
    "\n",
    "### forward:\n",
    "- in forward we will have\n",
    "```\n",
    "lstm_out, _ = self.LSTM(embeds.view(len(sentence), 1, -1))\n",
    "```\n",
    "- what exactly is going on here? Well, basically embeds is a tensor of dimensions (number of tokens in sentence, embedding dim) (**since rememember that the embedding layer takes each word index, and pulls up the corresponding row from the vocabulary)\n",
    "- we want to reshape it to (sequence length, batch_size, input_size)\n",
    "    - reshaping is: sequence length( the length of the sentence )\n",
    "    - batch size is 1, since its one sentence at a time \n",
    "    - input_size = -1: it assumes that the length will be embedding_dim \n",
    "- **embeds.view**: is a way to reshape in LSTM\n",
    "\n",
    "- forward in the encoder should only return the hidden state and cell state, since its what matters \n",
    "\n",
    "\n",
    "## backpropagation\n",
    "- pytorch under the hood tracks operations on tensors with require_grad = True. All nn.modules like nn.Linear and nn.LSTM already register their parameters with requires_grad=True, so as long as its connected correctly in the forward pass, pytorch will handle the gradients during backprop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder: English layer \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim) # initialize an LSTM, with embedding_dim, and hidden_dim hyperparameters \n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeddings(sentence)  # remember that sentence has to the in [word_index0, word_index1, word_index2] form\n",
    "        _, (h_n, c_n) = self.LSTM(embeds.view(len(sentence), 1, -1)) # one timestep at a tiem \n",
    "        return h_n, c_n\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[49, 617, 3692]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test example pass through the encoder \n",
    "encoder = Encoder(embedding_dim=3, vocab_size= len(en_vocab), hidden_dim=5)\n",
    "# now remember that for forward we pass a sentence as the list of words mapped to the indices they show up in the vocab, such as [45, 18, 28]\n",
    "sentence = \"I love bread.\"\n",
    "\n",
    "input_words = get_words_from_sentence(remove_punctuation(sentence).lower())\n",
    "# now map the inputs to the vocab \n",
    "input_indices = [en_vocab[word] for word in input_words] \n",
    "# now that I think about it, we probably want a function that does this, so that we don't get hit with a KeyError and actually use our <unk> token lul\n",
    "input_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.3679, -0.1684, -0.2184,  0.2481,  0.1513]]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " tensor([[[-0.6571, -0.4459, -0.4932,  0.3003,  0.7057]]],\n",
       "        grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with the input_indices, we can now throw it through the encoder?\n",
    "# oh wait yopu need tensor first lul\n",
    "input_indices_tensor = torch.tensor(input_indices, dtype=torch.long)\n",
    "output = encoder.forward(input_indices_tensor)\n",
    "output # this makes sense, we have both hidden and cell states :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "- so the decoder is another LSTM, taking as input \"a large fixed-dimensional vector representation\", and then use another LSTM to extract the output sequence from that vector \n",
    "- we can just pass in h_n and c_n in the decoder LSTM as parameter for this! \n",
    "- and then for forward we just run the linear layer and then run the log_softmax to get the logits?\n",
    "\n",
    "- What about the Embedding layer?\n",
    "    - we also need an embedding layer ( used in both training and inference )\n",
    "    - training phase we have \"ground-truth\" tokens, we need the \"ground-truth\" tokens we need the embedding layer to make them tensors and to feed each token through the decoder\n",
    "\n",
    "### Teacher forcing and backpropagation\n",
    "- At a time step t, the input is the actual target sequence from t - 1 !. This makes sense, we give it the \"correct\" input from the time before, and have it try to predict the input now.  the t-1 token is called the \"ground truth\" token, is passed through the embedding layer specifically trained for the target language vocabulary\n",
    "- the output is the predicted timestep t token, and what you use to compare as loss is the actual t token. \n",
    "\n",
    "**forward step in the decoder**\n",
    "- when making a prediction, you either use the correct previous token (teacher forcing during training), or you use the previous prediction (inference)\n",
    "- during prediction we will use nn.LSTM. You want to pass the token at t-1's tensor, AND the previous c_n and h_n from the decoder! it's the recurrent aspect of the RNN\n",
    "\n",
    "- we also do feed in the encoder h_n, and c_n, in the first step :).\n",
    "- don't need to worry about the rest, because it already does it implicitly by nn.LSTM\n",
    "\n",
    "- also to enforce teacher forcing, we have to do one time step at a time, instead of all at once \n",
    "    - so instead, we go through every single input one at time \n",
    "\n",
    "- in the no teacher YOU USE THE PREVIOUS PREDICTION OF THE TIME(after it is )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurations \n",
    "MAX_RESPONSE_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size) # define the linear layer \n",
    "    \n",
    "    def word_to_tensor(self, word):\n",
    "        '''\n",
    "        takes a single wrod and gets the corresponding tensor\n",
    "        '''\n",
    "        word_lst = get_words_from_sentence(remove_zh_punctuation(word))\n",
    "        indices = [zh_vocab[word] for word in word_lst]\n",
    "        # get tensor \n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def tensor_outputs_to_sentence(self, output_tensor):\n",
    "        s = ''\n",
    "        zh_vocab_lst = list(zh_vocab.keys())\n",
    "        for word_tensor in output_tensor:\n",
    "            pred_idx = torch.argmax(word_tensor, dim=-1).item()\n",
    "            s += zh_vocab_lst[pred_idx]\n",
    "        return s \n",
    "\n",
    "    def forward(self, hidden, sentence=None):\n",
    "        # if we pass in a sentence, then we are trying teacher forcing.\n",
    "        all_outputs = []\n",
    "        targets = None\n",
    "        if sentence is not None:\n",
    "            # target should not be embeddings \n",
    "            targets = sentence[1:]\n",
    "            embeds_lst = self.embeddings(sentence)\n",
    "            teacher_forcing_inputs = embeds_lst[:-1]\n",
    "            for i in range(len(teacher_forcing_inputs)):\n",
    "                # use embed to actually make the prediction\n",
    "                input_tensor = teacher_forcing_inputs[i]\n",
    "                out, hidden = self.LSTM(input_tensor.view(1, 1, -1), hidden)\n",
    "                logits = self.linear(out)\n",
    "                all_outputs.append(logits)\n",
    "        else:\n",
    "            # just generate some tokens, starting from the <sos> token\n",
    "            start_token = self.word_to_tensor('<SOS>')\n",
    "            # run through embedding layer\n",
    "            prev_char = start_token\n",
    "            for i in range(MAX_RESPONSE_LENGTH):\n",
    "                embeds = self.embeddings(prev_char)\n",
    "                out, hidden = self.LSTM(embeds.view(1, 1, -1), hidden)\n",
    "                logits = self.linear(out)\n",
    "                all_outputs.append(logits)\n",
    "                pred_idx = torch.argmax(logits, dim=2).item()\n",
    "                prev_char = torch.tensor(pred_idx, dtype=torch.long)\n",
    "            \n",
    "        return torch.cat(all_outputs, dim=0), targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions to take a sentence and turn it into a tensor, adding <sos> and <eos>\n",
    "def sequence_to_tensor_en(sequence):\n",
    "    '''\n",
    "    takes sequence and converts to tensor \n",
    "    '''\n",
    "    # add \"<SOS> and <EOS>\"\n",
    "    words = get_words_from_sentence(\"<SOS> \" + remove_punctuation(sequence).lower() + \" <EOS>\")\n",
    "    \n",
    "    # convert to indices, reverting to <UNK> token\n",
    "    word_indices = [ en_vocab[word] if word in en_vocab else en_vocab[\"<UNK>\"] for word in words ]\n",
    "    return torch.tensor(word_indices, dtype=torch.long)\n",
    "    \n",
    "\n",
    "def sequence_to_tensor_zh(sequence):\n",
    "    '''\n",
    "    takes sequence and converts to chinese tensor \n",
    "    '''\n",
    "    words = ([\"<SOS>\"] + list(remove_zh_punctuation(sequence)))\n",
    "    words.append(\"<EOS>\")\n",
    "    \n",
    "    word_indices = [ zh_vocab[word] if word in zh_vocab else zh_vocab[\"<UNK>\"] for word in words ]\n",
    "    return torch.tensor(word_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a full run through both the Encoder and the decoder \n",
    "\n",
    "encoder = Encoder(embedding_dim=3, vocab_size=len(en_vocab), hidden_dim=5)\n",
    "en_sentence = \"I love bread.\"\n",
    "zh_sentence = \"我爱面包\"\n",
    "\n",
    "\n",
    "h_n, c_n = encoder.forward(sequence_to_tensor_en(en_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'饥素务务素务奇趣素务'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the decoder\n",
    "decoder = Decoder(embedding_dim=3, vocab_size=len(zh_vocab), hidden_dim=5)\n",
    "predicted_sentence_tensor, target = decoder.forward((h_n, c_n))\n",
    "predicted_sentence = decoder.tensor_outputs_to_sentence(predicted_sentence_tensor)\n",
    "predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "湾aaaaaaaaa\n",
      "Number of trains 100\n",
      "Loss 7.089718408584595\n",
      "Number of trains 200\n",
      "Loss 6.967032151222229\n",
      "Number of trains 300\n",
      "Loss 6.939106192588806\n",
      "Number of trains 400\n",
      "Loss 7.019029750823974\n",
      "Number of trains 500\n",
      "Loss 6.846007628440857\n",
      "Number of trains 600\n",
      "Loss 6.969411320686341\n",
      "Number of trains 700\n",
      "Loss 6.819410881996155\n",
      "Number of trains 800\n",
      "Loss 6.821892666816711\n",
      "Number of trains 900\n",
      "Loss 6.908918623924255\n",
      "Number of trains 1000\n",
      "Loss 6.8386824464797975\n",
      "Number of trains 1100\n",
      "Loss 6.815266137123108\n",
      "Number of trains 1200\n",
      "Loss 6.831244988441467\n",
      "Number of trains 1300\n",
      "Loss 6.699757814407349\n",
      "Number of trains 1400\n",
      "Loss 6.811405534744263\n",
      "Number of trains 1500\n",
      "Loss 6.913129119873047\n",
      "Number of trains 1600\n",
      "Loss 6.795093812942505\n",
      "Number of trains 1700\n",
      "Loss 6.793145561218262\n",
      "Number of trains 1800\n",
      "Loss 6.830744276046753\n",
      "Number of trains 1900\n",
      "Loss 6.783864507675171\n",
      "Number of trains 2000\n",
      "Loss 6.836080665588379\n",
      "Number of trains 2100\n",
      "Loss 6.828772521018982\n",
      "Number of trains 2200\n",
      "Loss 6.94904860496521\n",
      "Number of trains 2300\n",
      "Loss 6.77602020740509\n",
      "Number of trains 2400\n",
      "Loss 6.8152975797653195\n",
      "Number of trains 2500\n",
      "Loss 6.776704297065735\n",
      "Number of trains 2600\n",
      "Loss 6.843909811973572\n",
      "Number of trains 2700\n",
      "Loss 6.905662989616394\n",
      "Number of trains 2800\n",
      "Loss 6.763120279312134\n",
      "Number of trains 2900\n",
      "Loss 6.812482204437256\n",
      "Number of trains 3000\n",
      "Loss 6.7496369314193725\n",
      "Number of trains 3100\n",
      "Loss 6.82880615234375\n",
      "Number of trains 3200\n",
      "Loss 6.839840250015259\n",
      "Number of trains 3300\n",
      "Loss 6.806010308265686\n",
      "Number of trains 3400\n",
      "Loss 6.77107232093811\n",
      "Number of trains 3500\n",
      "Loss 6.809851388931275\n",
      "Number of trains 3600\n",
      "Loss 6.8807019519805905\n",
      "Number of trains 3700\n",
      "Loss 6.7368270111083985\n",
      "Number of trains 3800\n",
      "Loss 6.753353886604309\n",
      "Number of trains 3900\n",
      "Loss 6.786076922416687\n",
      "Number of trains 4000\n",
      "Loss 6.79586100101471\n",
      "Number of trains 4100\n",
      "Loss 6.883830561637878\n",
      "Number of trains 4200\n",
      "Loss 6.778929796218872\n",
      "Number of trains 4300\n",
      "Loss 6.763175435066223\n",
      "Number of trains 4400\n",
      "Loss 6.754985470771789\n",
      "Number of trains 4500\n",
      "Loss 6.840541167259216\n",
      "Number of trains 4600\n",
      "Loss 6.819540805816651\n",
      "Number of trains 4700\n",
      "Loss 6.763791456222534\n",
      "Number of trains 4800\n",
      "Loss 6.84369653224945\n",
      "Number of trains 4900\n",
      "Loss 6.752576284408569\n",
      "Number of trains 5000\n",
      "Loss 6.873677797317505\n",
      "Number of trains 5100\n",
      "Loss 6.795124502182007\n",
      "Number of trains 5200\n",
      "Loss 6.8088716840744015\n",
      "Number of trains 5300\n",
      "Loss 6.8174641847610475\n",
      "Number of trains 5400\n",
      "Loss 6.817717785835266\n",
      "Number of trains 5500\n",
      "Loss 6.857389702796936\n",
      "Number of trains 5600\n",
      "Loss 6.822575135231018\n",
      "Number of trains 5700\n",
      "Loss 6.793008961677551\n",
      "Number of trains 5800\n",
      "Loss 6.870430889129639\n",
      "Number of trains 5900\n",
      "Loss 6.7880486440658565\n",
      "Number of trains 6000\n",
      "Loss 6.752993035316467\n",
      "Number of trains 6100\n",
      "Loss 6.832165288925171\n",
      "Number of trains 6200\n",
      "Loss 6.716077599525452\n",
      "Number of trains 6300\n",
      "Loss 6.815958199501037\n",
      "Number of trains 6400\n",
      "Loss 6.767104911804199\n",
      "Number of trains 6500\n",
      "Loss 6.753328137397766\n",
      "Number of trains 6600\n",
      "Loss 6.805204811096192\n",
      "Number of trains 6700\n",
      "Loss 6.802783932685852\n",
      "Number of trains 6800\n",
      "Loss 6.772719368934632\n",
      "Number of trains 6900\n",
      "Loss 6.858709506988525\n",
      "Number of trains 7000\n",
      "Loss 6.7640944719314575\n",
      "Number of trains 7100\n",
      "Loss 6.813980069160461\n",
      "Number of trains 7200\n",
      "Loss 6.771928277015686\n",
      "Number of trains 7300\n",
      "Loss 6.766301188468933\n",
      "Number of trains 7400\n",
      "Loss 6.715135207176209\n",
      "Number of trains 7500\n",
      "Loss 6.731577682495117\n",
      "Number of trains 7600\n",
      "Loss 6.778827810287476\n",
      "Number of trains 7700\n",
      "Loss 6.74675486087799\n",
      "Number of trains 7800\n",
      "Loss 6.826909656524658\n",
      "Number of trains 7900\n",
      "Loss 6.77967324256897\n",
      "Number of trains 8000\n",
      "Loss 6.768084297180176\n",
      "Number of trains 8100\n",
      "Loss 6.767041916847229\n",
      "Number of trains 8200\n",
      "Loss 6.721581144332886\n",
      "Number of trains 8300\n",
      "Loss 6.759984602928162\n",
      "Number of trains 8400\n",
      "Loss 6.798296875953675\n",
      "Number of trains 8500\n",
      "Loss 6.725043158531189\n",
      "Number of trains 8600\n",
      "Loss 6.736663980484009\n",
      "Number of trains 8700\n",
      "Loss 6.764224090576172\n",
      "Number of trains 8800\n",
      "Loss 6.846845297813416\n",
      "Number of trains 8900\n",
      "Loss 6.728883996009826\n",
      "Number of trains 9000\n",
      "Loss 6.729794063568115\n",
      "Number of trains 9100\n",
      "Loss 6.87550765991211\n",
      "Number of trains 9200\n",
      "Loss 6.767141137123108\n",
      "Number of trains 9300\n",
      "Loss 6.730149397850036\n",
      "Number of trains 9400\n",
      "Loss 6.76149685382843\n",
      "Number of trains 9500\n",
      "Loss 6.789661884307861\n",
      "Number of trains 9600\n",
      "Loss 6.762967720031738\n",
      "Number of trains 9700\n",
      "Loss 6.835050363540649\n",
      "Number of trains 9800\n",
      "Loss 6.836092300415039\n",
      "Number of trains 9900\n",
      "Loss 6.746917576789856\n",
      "Number of trains 10000\n",
      "Loss 6.748668417930603\n",
      "的的的的的的的的的的\n",
      "Number of trains 10100\n",
      "Loss 6.745449566841126\n",
      "Number of trains 10200\n",
      "Loss 6.73597544670105\n",
      "Number of trains 10300\n",
      "Loss 6.769990477561951\n",
      "Number of trains 10400\n",
      "Loss 6.82487530708313\n",
      "Number of trains 10500\n",
      "Loss 6.7506098985672\n",
      "Number of trains 10600\n",
      "Loss 6.853042945861817\n",
      "Number of trains 10700\n",
      "Loss 6.756102967262268\n",
      "Number of trains 10800\n",
      "Loss 6.744778323173523\n",
      "Number of trains 10900\n",
      "Loss 6.847487244606018\n",
      "Number of trains 11000\n",
      "Loss 6.760415244102478\n",
      "Number of trains 11100\n",
      "Loss 6.751785073280335\n",
      "Number of trains 11200\n",
      "Loss 6.772518811225891\n",
      "Number of trains 11300\n",
      "Loss 6.669023141860962\n",
      "Number of trains 11400\n",
      "Loss 6.782300510406494\n",
      "Number of trains 11500\n",
      "Loss 6.875701584815979\n",
      "Number of trains 11600\n",
      "Loss 6.753252243995666\n",
      "Number of trains 11700\n",
      "Loss 6.768581614494324\n",
      "Number of trains 11800\n",
      "Loss 6.799774441719055\n",
      "Number of trains 11900\n",
      "Loss 6.741528587341309\n",
      "Number of trains 12000\n",
      "Loss 6.787223896980286\n",
      "Number of trains 12100\n",
      "Loss 6.795571575164795\n",
      "Number of trains 12200\n",
      "Loss 6.937638578414917\n",
      "Number of trains 12300\n",
      "Loss 6.763757100105286\n",
      "Number of trains 12400\n",
      "Loss 6.801685695648193\n",
      "Number of trains 12500\n",
      "Loss 6.7738885498046875\n",
      "Number of trains 12600\n",
      "Loss 6.8358023071289065\n",
      "Number of trains 12700\n",
      "Loss 6.888618144989014\n",
      "Number of trains 12800\n",
      "Loss 6.7516419887542725\n",
      "Number of trains 12900\n",
      "Loss 6.801029210090637\n",
      "Number of trains 13000\n",
      "Loss 6.740972309112549\n",
      "Number of trains 13100\n",
      "Loss 6.818916220664978\n",
      "Number of trains 13200\n",
      "Loss 6.82833993434906\n",
      "Number of trains 13300\n",
      "Loss 6.801547694206238\n",
      "Number of trains 13400\n",
      "Loss 6.764651937484741\n",
      "Number of trains 13500\n",
      "Loss 6.804521508216858\n",
      "Number of trains 13600\n",
      "Loss 6.868497409820557\n",
      "Number of trains 13700\n",
      "Loss 6.726677398681641\n",
      "Number of trains 13800\n",
      "Loss 6.759438400268555\n",
      "Number of trains 13900\n",
      "Loss 6.786586012840271\n",
      "Number of trains 14000\n",
      "Loss 6.791972694396972\n",
      "Number of trains 14100\n",
      "Loss 6.871897196769714\n",
      "Number of trains 14200\n",
      "Loss 6.774816489219665\n",
      "Number of trains 14300\n",
      "Loss 6.761291975975037\n",
      "Number of trains 14400\n",
      "Loss 6.754576606750488\n",
      "Number of trains 14500\n",
      "Loss 6.832817025184632\n",
      "Number of trains 14600\n",
      "Loss 6.817082118988037\n",
      "Number of trains 14700\n",
      "Loss 6.760182061195374\n",
      "Number of trains 14800\n",
      "Loss 6.838723454475403\n",
      "Number of trains 14900\n",
      "Loss 6.74776825428009\n",
      "Number of trains 15000\n",
      "Loss 6.871434864997863\n",
      "Number of trains 15100\n",
      "Loss 6.79359990119934\n",
      "Number of trains 15200\n",
      "Loss 6.807861394882202\n",
      "Number of trains 15300\n",
      "Loss 6.815065188407898\n",
      "Number of trains 15400\n",
      "Loss 6.813795156478882\n",
      "Number of trains 15500\n",
      "Loss 6.858024520874023\n",
      "Number of trains 15600\n",
      "Loss 6.831940231323242\n",
      "Number of trains 15700\n",
      "Loss 6.812987265586853\n",
      "Number of trains 15800\n",
      "Loss 6.875465745925903\n",
      "Number of trains 15900\n",
      "Loss 6.7901465654373165\n",
      "Number of trains 16000\n",
      "Loss 6.758711838722229\n",
      "Number of trains 16100\n",
      "Loss 6.840643358230591\n",
      "Number of trains 16200\n",
      "Loss 6.728657927513122\n",
      "Number of trains 16300\n",
      "Loss 6.836826648712158\n",
      "Number of trains 16400\n",
      "Loss 6.774903106689453\n",
      "Number of trains 16500\n",
      "Loss 6.768968868255615\n",
      "Number of trains 16600\n",
      "Loss 6.810204916000366\n",
      "Number of trains 16700\n",
      "Loss 6.818220381736755\n",
      "Number of trains 16800\n",
      "Loss 6.786173815727234\n",
      "Number of trains 16900\n",
      "Loss 6.870270538330078\n",
      "Number of trains 17000\n",
      "Loss 6.78566752910614\n",
      "Number of trains 17100\n",
      "Loss 6.8165002536773684\n",
      "Number of trains 17200\n",
      "Loss 6.7994555521011355\n",
      "Number of trains 17300\n",
      "Loss 6.778291215896607\n",
      "Number of trains 17400\n",
      "Loss 6.73934380531311\n",
      "Number of trains 17500\n",
      "Loss 6.755999979972839\n",
      "Number of trains 17600\n",
      "Loss 6.796342487335205\n",
      "Number of trains 17700\n",
      "Loss 6.771689248085022\n",
      "Number of trains 17800\n",
      "Loss 6.844671449661255\n",
      "Number of trains 17900\n",
      "Loss 6.805525951385498\n",
      "Number of trains 18000\n",
      "Loss 6.787480654716492\n",
      "Number of trains 18100\n",
      "Loss 6.781659932136535\n",
      "Number of trains 18200\n",
      "Loss 6.745037455558776\n",
      "Number of trains 18300\n",
      "Loss 6.779730052947998\n",
      "Number of trains 18400\n",
      "Loss 6.806529364585876\n",
      "Number of trains 18500\n",
      "Loss 6.731444425582886\n",
      "Number of trains 18600\n",
      "Loss 6.771145000457763\n",
      "Number of trains 18700\n",
      "Loss 6.795873031616211\n",
      "Number of trains 18800\n",
      "Loss 6.8601807117462155\n",
      "Number of trains 18900\n",
      "Loss 6.740870356559753\n",
      "Number of trains 19000\n",
      "Loss 6.745765252113342\n",
      "Number of trains 19100\n",
      "Loss 6.894409523010254\n",
      "Number of trains 19200\n",
      "Loss 6.78119969367981\n",
      "Number of trains 19300\n",
      "Loss 6.753666090965271\n",
      "Number of trains 19400\n",
      "Loss 6.769359788894653\n",
      "Number of trains 19500\n",
      "Loss 6.8006898355484005\n",
      "Number of trains 19600\n",
      "Loss 6.775432395935058\n",
      "Number of trains 19700\n",
      "Loss 6.843854637145996\n",
      "Number of trains 19800\n",
      "Loss 6.864270868301392\n",
      "Number of trains 19900\n",
      "Loss 6.781409583091736\n",
      "Number of trains 20000\n",
      "Loss 6.756898827552796\n",
      "的的的的的的的的的的\n",
      "Number of trains 20100\n",
      "Loss 6.7781882572174075\n",
      "Number of trains 20200\n",
      "Loss 6.765395336151123\n",
      "Number of trains 20300\n",
      "Loss 6.788004760742187\n",
      "Number of trains 20400\n",
      "Loss 6.858287186622619\n",
      "Number of trains 20500\n",
      "Loss 6.771993045806885\n",
      "Number of trains 20600\n",
      "Loss 6.882263555526733\n",
      "Number of trains 20700\n",
      "Loss 6.76554012298584\n",
      "Number of trains 20800\n",
      "Loss 6.774259214401245\n",
      "Number of trains 20900\n",
      "Loss 6.848517479896546\n",
      "Number of trains 21000\n",
      "Loss 6.798730058670044\n",
      "Number of trains 21100\n",
      "Loss 6.787678074836731\n",
      "Number of trains 21200\n",
      "Loss 6.805908069610596\n",
      "Number of trains 21300\n",
      "Loss 6.693189296722412\n",
      "Number of trains 21400\n",
      "Loss 6.795393648147583\n",
      "Number of trains 21500\n",
      "Loss 6.893863024711609\n",
      "Number of trains 21600\n",
      "Loss 6.775662851333618\n",
      "Number of trains 21700\n",
      "Loss 6.7793041563034055\n",
      "Number of trains 21800\n",
      "Loss 6.815054550170898\n",
      "Number of trains 21900\n",
      "Loss 6.769965925216675\n",
      "Number of trains 22000\n",
      "Loss 6.816638116836548\n",
      "Number of trains 22100\n",
      "Loss 6.811201257705688\n",
      "Number of trains 22200\n",
      "Loss 6.93692608833313\n",
      "Number of trains 22300\n",
      "Loss 6.762027039527893\n",
      "Number of trains 22400\n",
      "Loss 6.799129381179809\n",
      "Number of trains 22500\n",
      "Loss 6.772401175498962\n",
      "Number of trains 22600\n",
      "Loss 6.835641331672669\n",
      "Number of trains 22700\n",
      "Loss 6.88815670967102\n",
      "Number of trains 22800\n",
      "Loss 6.7509805917739865\n",
      "Number of trains 22900\n",
      "Loss 6.801133456230164\n",
      "Number of trains 23000\n",
      "Loss 6.739721088409424\n",
      "Number of trains 23100\n",
      "Loss 6.819180703163147\n",
      "Number of trains 23200\n",
      "Loss 6.827975969314576\n",
      "Number of trains 23300\n",
      "Loss 6.801724190711975\n",
      "Number of trains 23400\n",
      "Loss 6.764727058410645\n",
      "Number of trains 23500\n",
      "Loss 6.8045852708816525\n",
      "Number of trains 23600\n",
      "Loss 6.868427658081055\n",
      "Number of trains 23700\n",
      "Loss 6.72674144744873\n",
      "Number of trains 23800\n",
      "Loss 6.759499473571777\n",
      "Number of trains 23900\n",
      "Loss 6.786552376747132\n",
      "Number of trains 24000\n",
      "Loss 6.791997880935669\n",
      "Number of trains 24100\n",
      "Loss 6.871906843185425\n",
      "Number of trains 24200\n",
      "Loss 6.774827003479004\n",
      "Number of trains 24300\n",
      "Loss 6.7612695264816285\n",
      "Number of trains 24400\n",
      "Loss 6.7546078491210935\n",
      "Number of trains 24500\n",
      "Loss 6.832766256332397\n",
      "Number of trains 24600\n",
      "Loss 6.8170840406417845\n",
      "Number of trains 24700\n",
      "Loss 6.760189118385315\n",
      "Number of trains 24800\n",
      "Loss 6.838698582649231\n",
      "Number of trains 24900\n",
      "Loss 6.747775392532349\n",
      "Number of trains 25000\n",
      "Loss 6.871437635421753\n",
      "Number of trains 25100\n",
      "Loss 6.793577389717102\n",
      "Number of trains 25200\n",
      "Loss 6.807861380577087\n",
      "Number of trains 25300\n",
      "Loss 6.8184308910369875\n",
      "Number of trains 25400\n",
      "Loss 6.813717274665833\n",
      "Number of trains 25500\n",
      "Loss 6.858022470474243\n",
      "Number of trains 25600\n",
      "Loss 6.831938018798828\n",
      "Number of trains 25700\n",
      "Loss 6.813001656532288\n",
      "Number of trains 25800\n",
      "Loss 6.8754739093780515\n",
      "Number of trains 25900\n",
      "Loss 6.790150017738342\n",
      "Number of trains 26000\n",
      "Loss 6.758719997406006\n",
      "Number of trains 26100\n",
      "Loss 6.840647912025451\n",
      "Number of trains 26200\n",
      "Loss 6.7286587333679195\n",
      "Number of trains 26300\n",
      "Loss 6.836828927993775\n",
      "Number of trains 26400\n",
      "Loss 6.77490387916565\n",
      "Number of trains 26500\n",
      "Loss 6.768969287872315\n",
      "Number of trains 26600\n",
      "Loss 6.809950394630432\n",
      "Number of trains 26700\n",
      "Loss 6.818220825195312\n",
      "Number of trains 26800\n",
      "Loss 6.786258840560913\n",
      "Number of trains 26900\n",
      "Loss 6.87102108001709\n",
      "Number of trains 27000\n",
      "Loss 6.787995462417602\n",
      "Number of trains 27100\n",
      "Loss 6.816726603507996\n",
      "Number of trains 27200\n",
      "Loss 6.799451832771301\n",
      "Number of trains 27300\n",
      "Loss 6.778304982185364\n",
      "Number of trains 27400\n",
      "Loss 6.739341621398926\n",
      "Number of trains 27500\n",
      "Loss 6.756006565093994\n",
      "Number of trains 27600\n",
      "Loss 6.796351590156555\n",
      "Number of trains 27700\n",
      "Loss 6.771693429946899\n",
      "Number of trains 27800\n",
      "Loss 6.844675464630127\n",
      "Number of trains 27900\n",
      "Loss 6.8055334615707395\n",
      "Number of trains 28000\n",
      "Loss 6.7874902153015135\n",
      "Number of trains 28100\n",
      "Loss 6.781663341522217\n",
      "Number of trains 28200\n",
      "Loss 6.745039086341858\n",
      "Number of trains 28300\n",
      "Loss 6.779738330841065\n",
      "Number of trains 28400\n",
      "Loss 6.806531767845154\n",
      "Number of trains 28500\n",
      "Loss 6.731446876525879\n",
      "Number of trains 28600\n",
      "Loss 6.771144671440124\n",
      "Number of trains 28700\n",
      "Loss 6.795872888565063\n",
      "Number of trains 28800\n",
      "Loss 6.860183787345886\n",
      "Number of trains 28900\n",
      "Loss 6.740869579315185\n",
      "Number of trains 29000\n",
      "Loss 6.745766310691834\n",
      "Number of trains 29100\n",
      "Loss 6.89441035270691\n",
      "Number of trains 29200\n",
      "Loss 6.781200995445252\n",
      "Number of trains 29300\n",
      "Loss 6.7536667585372925\n",
      "Number of trains 29400\n",
      "Loss 6.765808897018433\n",
      "Number of trains 29500\n",
      "Loss 6.804259557723999\n",
      "Number of trains 29600\n",
      "Loss 6.773634519577026\n",
      "Number of trains 29700\n",
      "Loss 6.847323622703552\n",
      "Number of trains 29800\n",
      "Loss 6.863019943237305\n",
      "Number of trains 29900\n",
      "Loss 6.780839757919312\n",
      "Number of trains 30000\n",
      "Loss 6.757072863578796\n",
      "的的的的的的的的的的\n",
      "Number of trains 30100\n",
      "Loss 6.7779865550994876\n",
      "Number of trains 30200\n",
      "Loss 6.765359916687012\n",
      "Number of trains 30300\n",
      "Loss 6.788030385971069\n",
      "Number of trains 30400\n",
      "Loss 6.858283247947693\n",
      "Number of trains 30500\n",
      "Loss 6.772005867958069\n",
      "Number of trains 30600\n",
      "Loss 6.882251553535461\n",
      "Number of trains 30700\n",
      "Loss 6.765544066429138\n",
      "Number of trains 30800\n",
      "Loss 6.774257440567016\n",
      "Number of trains 30900\n",
      "Loss 6.848520455360412\n",
      "Number of trains 31000\n",
      "Loss 6.798740329742432\n",
      "Number of trains 31100\n",
      "Loss 6.7876833152771\n",
      "Number of trains 31200\n",
      "Loss 6.805913133621216\n",
      "Number of trains 31300\n",
      "Loss 6.693192167282104\n",
      "Number of trains 31400\n",
      "Loss 6.795399055480957\n",
      "Number of trains 31500\n",
      "Loss 6.893865036964416\n",
      "Number of trains 31600\n",
      "Loss 6.775667161941528\n",
      "Number of trains 31700\n",
      "Loss 6.779305033683777\n",
      "Number of trains 31800\n",
      "Loss 6.815057430267334\n",
      "Number of trains 31900\n",
      "Loss 6.769965753555298\n",
      "Number of trains 32000\n",
      "Loss 6.816639542579651\n",
      "Number of trains 32100\n",
      "Loss 6.811202445030212\n",
      "Number of trains 32200\n",
      "Loss 6.936927156448364\n",
      "Number of trains 32300\n",
      "Loss 6.762028722763062\n",
      "Number of trains 32400\n",
      "Loss 6.799129266738891\n",
      "Number of trains 32500\n",
      "Loss 6.772401952743531\n",
      "Number of trains 32600\n",
      "Loss 6.8356419992446895\n",
      "Number of trains 32700\n",
      "Loss 6.888157358169556\n",
      "Number of trains 32800\n",
      "Loss 6.750981378555298\n",
      "Number of trains 32900\n",
      "Loss 6.801133861541748\n",
      "Number of trains 33000\n",
      "Loss 6.739721026420593\n",
      "Number of trains 33100\n",
      "Loss 6.819181289672851\n",
      "Number of trains 33200\n",
      "Loss 6.827976403236389\n",
      "Number of trains 33300\n",
      "Loss 6.801724328994751\n",
      "Number of trains 33400\n",
      "Loss 6.764727101325989\n",
      "Number of trains 33500\n",
      "Loss 6.804585394859314\n",
      "Number of trains 33600\n",
      "Loss 6.8684279823303225\n",
      "Number of trains 33700\n",
      "Loss 6.726741642951965\n",
      "Number of trains 33800\n",
      "Loss 6.759499793052673\n",
      "Number of trains 33900\n",
      "Loss 6.786552276611328\n",
      "Number of trains 34000\n",
      "Loss 6.791998085975647\n",
      "Number of trains 34100\n",
      "Loss 6.871906900405884\n",
      "Number of trains 34200\n",
      "Loss 6.774827156066895\n",
      "Number of trains 34300\n",
      "Loss 6.76126944065094\n",
      "Number of trains 34400\n",
      "Loss 6.754607515335083\n",
      "Number of trains 34500\n",
      "Loss 6.83276629447937\n",
      "Number of trains 34600\n",
      "Loss 6.817084016799927\n",
      "Number of trains 34700\n",
      "Loss 6.760189146995544\n",
      "Number of trains 34800\n",
      "Loss 6.838698735237122\n",
      "Number of trains 34900\n",
      "Loss 6.747775435447693\n",
      "Number of trains 35000\n",
      "Loss 6.871437606811523\n",
      "Number of trains 35100\n",
      "Loss 6.793577389717102\n",
      "Number of trains 35200\n",
      "Loss 6.807837924957275\n",
      "Number of trains 35300\n",
      "Loss 6.8150733470916744\n",
      "Number of trains 35400\n",
      "Loss 6.813802313804627\n",
      "Number of trains 35500\n",
      "Loss 6.858009276390075\n",
      "Number of trains 35600\n",
      "Loss 6.831938667297363\n",
      "Number of trains 35700\n",
      "Loss 6.8129819917678835\n",
      "Number of trains 35800\n",
      "Loss 6.875469727516174\n",
      "Number of trains 35900\n",
      "Loss 6.790137286186218\n",
      "Number of trains 36000\n",
      "Loss 6.75871693611145\n",
      "Number of trains 36100\n",
      "Loss 6.8406439304351805\n",
      "Number of trains 36200\n",
      "Loss 6.728657832145691\n",
      "Number of trains 36300\n",
      "Loss 6.836824860572815\n",
      "Number of trains 36400\n",
      "Loss 6.774903898239136\n",
      "Number of trains 36500\n",
      "Loss 6.76896948337555\n",
      "Number of trains 36600\n",
      "Loss 6.809941682815552\n",
      "Number of trains 36700\n",
      "Loss 6.818220911026001\n",
      "Number of trains 36800\n",
      "Loss 6.786174302101135\n",
      "Number of trains 36900\n",
      "Loss 6.870257949829101\n",
      "Number of trains 37000\n",
      "Loss 6.785659275054932\n",
      "Number of trains 37100\n",
      "Loss 6.816502933502197\n",
      "Number of trains 37200\n",
      "Loss 6.799456820487976\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Encoder(embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(en_vocab), hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     39\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(zh_vocab), hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set_mini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[67], line 30\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, training_data, encoder, decoder, lr)\u001b[0m\n\u001b[1;32m     28\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     29\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 30\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m hidden2 \u001b[38;5;241m=\u001b[39m encoder\u001b[38;5;241m.\u001b[39mforward(sequence_to_tensor_en(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love bread\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(num_epochs, training_data, encoder, decoder, lr=0.1):\n",
    "    optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()), lr=lr\n",
    ")\n",
    "    count = 0\n",
    "    # do a \n",
    "    hidden = encoder.forward(sequence_to_tensor_en(\"I love bread\"))\n",
    "    predicted_tensor, target = decoder.forward(hidden)\n",
    "    print(decoder.tensor_outputs_to_sentence(predicted_tensor))\n",
    "    total_loss = 0\n",
    "    for i in range(num_epochs):\n",
    "        for pair in training_data:\n",
    "            count += 1\n",
    "            if count % 100 == 0:\n",
    "                print(f\"Number of trains {count}\")\n",
    "                # print the loss \n",
    "                print(f\"Loss {total_loss / 100}\")\n",
    "                total_loss = 0\n",
    "            english = pair['english']\n",
    "            zh = pair['chinese']\n",
    "            en_tensor = sequence_to_tensor_en(english)\n",
    "            zh_tensor = sequence_to_tensor_zh(zh)\n",
    "\n",
    "            h_n, c_n = encoder.forward(en_tensor)\n",
    "            predicted, target = decoder.forward((h_n, c_n), zh_tensor)\n",
    "            \n",
    "            loss = nn.functional.cross_entropy(torch.squeeze(predicted), target)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        hidden2 = encoder.forward(sequence_to_tensor_en(\"I love bread\"))\n",
    "        predicted_tensor2, target2 = decoder.forward(hidden2)\n",
    "        print(decoder.tensor_outputs_to_sentence(predicted_tensor2))\n",
    "\n",
    "    \n",
    "\n",
    "encoder = Encoder(embedding_dim=3, vocab_size=len(en_vocab), hidden_dim=5)\n",
    "decoder = Decoder(embedding_dim=3, vocab_size=len(zh_vocab), hidden_dim=5)\n",
    "\n",
    "train(50, train_set_mini, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "的的的的的的的的的的\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "的的的的的的的的的的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
