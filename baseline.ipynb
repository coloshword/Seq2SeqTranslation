{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq model for English to Chinese translation (baseline model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing, loading, and cleaning dataset\n",
    "\n",
    "- going to use datasets from: https://www.kaggle.com/datasets/qianhuan/translation?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5161434\n",
      "{'english': 'For greater sharpness, but with a slight increase in graininess, you can use a 1:1 dilution of this developer.', 'chinese': '为了更好的锐度，但是附带的会多一些颗粒度，可以使用这个显影剂的1：1稀释液。'}\n"
     ]
    }
   ],
   "source": [
    "train_set_path = \"dataset/translation2019zh_train.json\"\n",
    "\n",
    "train_set = []\n",
    "with open(train_set_path) as f:\n",
    "    for line in f:\n",
    "        train_set.append(json.loads(line))\n",
    "\n",
    "print(len(train_set))\n",
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to lower the size of this dataset, for testing purposes.\n",
    "- right now 5.1M sentences\n",
    "- let's make it 10,000 sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 'In fact, it is less clear-cut than that.', 'chinese': '但事实上并没有这么清晰了当。'}\n"
     ]
    }
   ],
   "source": [
    "# get 10,000 random indices \n",
    "sampled_indices = np.random.choice(len(train_set), 200000)\n",
    "\n",
    "train_subset = [train_set[i] for i in sampled_indices]\n",
    "print(train_subset[0])\n",
    "with open('dataset/train_set_mini.pkl', 'wb') as f:\n",
    "    pickle.dump(train_subset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 'In fact, it is less clear-cut than that.', 'chinese': '但事实上并没有这么清晰了当。'}\n"
     ]
    }
   ],
   "source": [
    "with open('dataset/train_set_mini.pkl', 'rb') as f:\n",
    "    train_set_mini = pickle.load(f)\n",
    "\n",
    "print(train_set_mini[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train set made. \n",
    "Now to work on the actual architecture\n",
    "\n",
    "## Preprocessing steps:\n",
    "- we want to maintain vocabulary for english and chinese. \n",
    "    - this is simple, just use a counter() and then limit it to if it appears within 5\n",
    "\n",
    "- In terms of encoding, we want to use sequence input, so a sentence becomes a list [3, 100, 8, 9], where each number corresponds to the index of the word in the dictionary\n",
    "    - we want to do this because LSTMs keep track of the word relationships at the sentence level\n",
    "    - then use nn.Embedding?\n",
    "        - nn.Embedding allows us to create a matrix representing the vocabulary. It allows us to train some nuance into the words, where instead of typical BoW where each word is just an index, each word is now a vector, which allows us to train some meaning into the word\n",
    "        - the embedding is a matrix size (vocab length, dim). Length vocab length because each row in the matrix corresponds to a word in the vocab, ( row of index = index of word in vocab :) )\n",
    "        \n",
    "\n",
    "# now to work on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions \n",
    "def remove_punctuation(text):\n",
    "    '''\n",
    "    Get rid of all punctuation from string text\n",
    "    '''\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def get_words_from_sentence(s):\n",
    "    '''\n",
    "    Gets words from sentence \n",
    "    '''\n",
    "    return s.split(' ')\n",
    "\n",
    "def clean_en_pair(pair):\n",
    "    '''\n",
    "    Cleans the english from the pair \n",
    "    '''\n",
    "    return get_words_from_sentence(remove_punctuation(pair['english']).lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words pre-clean 3828342\n",
      "Words post-clean 3808706\n",
      "31568\n",
      "6559724\n",
      "5613\n"
     ]
    }
   ],
   "source": [
    "def get_en_vocab(train_set):\n",
    "    '''\n",
    "    get_en_dict:\n",
    "        Gets an english vocab from train_set as a dict \n",
    "    '''\n",
    "    # get only the english sentences, list of strings \n",
    "    en_sentences = [clean_en_pair(pair) for pair in train_set]\n",
    "    en_sentences_flattened = [word for sentence in en_sentences for word in sentence]\n",
    "    print(f\"Words pre-clean {len(en_sentences_flattened)}\")\n",
    "    en_sentences_flattened = [word for word in en_sentences_flattened if word != '']\n",
    "    print(f\"Words post-clean {len(en_sentences_flattened)}\")\n",
    "    \n",
    "    word_counts = Counter(en_sentences_flattened)\n",
    "    # with word counts, now we limit the vocabulary to words that happen at least 5 times\n",
    "    en_vocab = {}\n",
    "    # {word: index}\n",
    "    idx = 0\n",
    "    for word in [\"<SOS>\", \"<EOS>\", \"<UNK>\"]:\n",
    "        en_vocab[word] = idx \n",
    "        idx += 1\n",
    "    for word, occurrences in word_counts.items():\n",
    "        if occurrences >= 5:\n",
    "            en_vocab[word] = idx \n",
    "            idx += 1\n",
    "    return en_vocab\n",
    "\n",
    "def remove_zh_punctuation(text):\n",
    "    cleaned = re.sub(r'[，。！？【】（）《》“”‘’、]', '', text)\n",
    "    cleaned = re.sub(r'\\s+', '', cleaned)\n",
    "    return cleaned\n",
    "\n",
    "def get_zh_vocab(train_set):\n",
    "    '''\n",
    "    get_zh_vocab:\n",
    "        Gets an zh vocab from train_set as a dict \n",
    "    '''\n",
    "    zh_sentences = [list(remove_zh_punctuation(pair['chinese'])) for pair in train_set]\n",
    "    zh_sentences_flattened = [word for sentence in zh_sentences for word in sentence]\n",
    "    print(len(zh_sentences_flattened))\n",
    "\n",
    "    word_counts = Counter(zh_sentences_flattened)\n",
    "    zh_vocab = {}\n",
    "\n",
    "    idx = 0 \n",
    "    for word in [\"<SOS>\", \"<EOS>\", \"<UNK>\"]:\n",
    "        zh_vocab[word] = idx \n",
    "        idx += 1 \n",
    "    for word, occurrences in word_counts.items():\n",
    "        if occurrences >= 2: \n",
    "            zh_vocab[word] = idx \n",
    "            idx += 1 \n",
    "    return zh_vocab\n",
    "\n",
    "en_vocab = get_en_vocab(train_set_mini)\n",
    "print(len(en_vocab))\n",
    "\n",
    "zh_vocab = get_zh_vocab(train_set_mini)\n",
    "print(len(zh_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab/en_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(en_vocab, f)\n",
    "\n",
    "with open('vocab/zh_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(zh_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab/en_vocab.pkl', 'rb') as f:\n",
    "    en_vocab = pickle.load(f)\n",
    "\n",
    "with open('vocab/zh_vocab.pkl', 'rb') as f:\n",
    "    zh_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture building\n",
    "- 2 LSTM's are the backbone\n",
    "- also build a higher level Seq2Seq model as abstraction of the entire model \n",
    "- nn.Embedding() as a variable for both Encoder and Decoder \n",
    "    - use vocab_size as row length, by the embedding dim as the column length\n",
    "- Encoder will be english, decoder will be chinese "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.LSTM\n",
    "\n",
    "- sequence models are central to NLPl they are models where there is some sort of dependence through teime between inputs. \n",
    "- a recurrent neural network is a network that maintains some kind of state.\n",
    "- for example its output could be used as part of the next input, so that information c an propagate along as the network passes over the sequence.\n",
    "- In the case of an LSTM, for each element in the sequence, there is a corresponding hidden state ht, which in principle contains infromation from arbitrary points earlier in the sequence. \n",
    "- we can use the hidden state to predict words in a language model, pos, and a myriad of oether things.\n",
    "\n",
    "LSTMs in pytorch:\n",
    "- pytorch LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The first axis is the sequence itself, the second indixes instances in the mini batch, and the third indexes elements of the input. \n",
    "- ignore mini batching, we will always just have 1 dimension on th second axis.\n",
    "- If we want to run the sequence model over the sentence \"The cow jumped\" our input should look like:\n",
    "\n",
    "[\n",
    "    q (the)\n",
    "    q (cow)\n",
    "    q (jumped)\n",
    "]\n",
    "Except remember there is an additional 2nd dimension with size 1, (this dimension )\n",
    "\n",
    "Initializing an LSTM:\n",
    "```python\n",
    "lstm = nn.LSTM(3, 3) #input dim is 3, output dim is 3 \n",
    "```\n",
    "\n",
    "input_size = 3: this means each input vector at a time step is of length 3. All inputs must have 3 columns, (n x 3). \n",
    "- each sequence = a list of input vectors (one per timestemp)\n",
    "- each input vector = size input_size \n",
    "\n",
    "input_size = 3, then your input tensor shape for 1 batch would be:\n",
    "    (seq_len, batch_size, 3)\n",
    "\n",
    "- What does this mean for the embedding layer?\n",
    "    - it must also be dimension (vocab_size, 3), since each token is mapped to a vecotr using the embedding. This embedding becomes the input at each timestep for the LSTM, and the LSTM accepts vector of dimensions (3).\n",
    "\n",
    "- using nn.LSTM example:\n",
    "    \n",
    "```python\n",
    "self.lstm = nn.LSTM(embedding_dim, hidden_dim)  #(embedding_dim) is the dimension of the embedding dim, and then hidden_dim is also essentially a hyperparameter, it's the dimension of the hidden state \n",
    "```\n",
    "\n",
    "**For most LSTM applications we will need a linear layer to learn the mapping from hidden state to the tag space. but in the case of the encoder, you don't need it! Since the linear layer is essentially the classifier layer, that learns to \n",
    "interpret the hidden layer.**\n",
    "\n",
    "\n",
    "### forward:\n",
    "- in forward we will have\n",
    "```\n",
    "lstm_out, _ = self.LSTM(embeds.view(len(sentence), 1, -1))\n",
    "```\n",
    "- what exactly is going on here? Well, basically embeds is a tensor of dimensions (number of tokens in sentence, embedding dim) (**since rememember that the embedding layer takes each word index, and pulls up the corresponding row from the vocabulary)\n",
    "- we want to reshape it to (sequence length, batch_size, input_size)\n",
    "    - reshaping is: sequence length( the length of the sentence )\n",
    "    - batch size is 1, since its one sentence at a time \n",
    "    - input_size = -1: it assumes that the length will be embedding_dim \n",
    "- **embeds.view**: is a way to reshape in LSTM\n",
    "\n",
    "- forward in the encoder should only return the hidden state and cell state, since its what matters \n",
    "\n",
    "\n",
    "## backpropagation\n",
    "- pytorch under the hood tracks operations on tensors with require_grad = True. All nn.modules like nn.Linear and nn.LSTM already register their parameters with requires_grad=True, so as long as its connected correctly in the forward pass, pytorch will handle the gradients during backprop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder: English layer \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim) # initialize an LSTM, with embedding_dim, and hidden_dim hyperparameters \n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeddings(sentence)  # remember that sentence has to the in [word_index0, word_index1, word_index2] form\n",
    "        _, (h_n, c_n) = self.LSTM(embeds.view(len(sentence), 1, -1)) # one timestep at a tiem \n",
    "        return h_n, c_n\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_words_from_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# now remember that for forward we pass a sentence as the list of words mapped to the indices they show up in the vocab, such as [45, 18, 28]\u001b[39;00m\n\u001b[1;32m      4\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love bread.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m input_words \u001b[38;5;241m=\u001b[39m \u001b[43mget_words_from_sentence\u001b[49m(remove_punctuation(sentence)\u001b[38;5;241m.\u001b[39mlower())\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# now map the inputs to the vocab \u001b[39;00m\n\u001b[1;32m      8\u001b[0m input_indices \u001b[38;5;241m=\u001b[39m [en_vocab[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m input_words] \n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_words_from_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "## Test example pass through the encoder \n",
    "encoder = Encoder(embedding_dim=3, vocab_size= len(en_vocab), hidden_dim=5)\n",
    "# now remember that for forward we pass a sentence as the list of words mapped to the indices they show up in the vocab, such as [45, 18, 28]\n",
    "sentence = \"I love bread.\"\n",
    "\n",
    "input_words = get_words_from_sentence(remove_punctuation(sentence).lower())\n",
    "# now map the inputs to the vocab \n",
    "input_indices = [en_vocab[word] for word in input_words] \n",
    "# now that I think about it, we probably want a function that does this, so that we don't get hit with a KeyError and actually use our <unk> token lul\n",
    "input_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.0234,  0.0432, -0.0121,  0.1221, -0.2183]]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " tensor([[[ 0.1151,  0.1039, -0.0202,  0.2092, -0.3026]]],\n",
       "        grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with the input_indices, we can now throw it through the encoder?\n",
    "# oh wait yopu need tensor first lul\n",
    "input_indices_tensor = torch.tensor(input_indices, dtype=torch.long)\n",
    "output = encoder.forward(input_indices_tensor)\n",
    "output # this makes sense, we have both hidden and cell states :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "- so the decoder is another LSTM, taking as input \"a large fixed-dimensional vector representation\", and then use another LSTM to extract the output sequence from that vector \n",
    "- we can just pass in h_n and c_n in the decoder LSTM as parameter for this! \n",
    "- and then for forward we just run the linear layer and then run the log_softmax to get the logits?\n",
    "\n",
    "- What about the Embedding layer?\n",
    "    - we also need an embedding layer ( used in both training and inference )\n",
    "    - training phase we have \"ground-truth\" tokens, we need the \"ground-truth\" tokens we need the embedding layer to make them tensors and to feed each token through the decoder\n",
    "\n",
    "### Teacher forcing and backpropagation\n",
    "- At a time step t, the input is the actual target sequence from t - 1 !. This makes sense, we give it the \"correct\" input from the time before, and have it try to predict the input now.  the t-1 token is called the \"ground truth\" token, is passed through the embedding layer specifically trained for the target language vocabulary\n",
    "- the output is the predicted timestep t token, and what you use to compare as loss is the actual t token. \n",
    "\n",
    "**forward step in the decoder**\n",
    "- when making a prediction, you either use the correct previous token (teacher forcing during training), or you use the previous prediction (inference)\n",
    "- during prediction we will use nn.LSTM. You want to pass the token at t-1's tensor, AND the previous c_n and h_n from the decoder! it's the recurrent aspect of the RNN\n",
    "\n",
    "- we also do feed in the encoder h_n, and c_n, in the first step :).\n",
    "- don't need to worry about the rest, because it already does it implicitly by nn.LSTM\n",
    "\n",
    "- also to enforce teacher forcing, we have to do one time step at a time, instead of all at once \n",
    "    - so instead, we go through every single input one at time \n",
    "\n",
    "- in the no teacher YOU USE THE PREVIOUS PREDICTION OF THE TIME(after it is )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurations \n",
    "MAX_RESPONSE_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_dim, device):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.device = device\n",
    "\n",
    "    def word_to_tensor(self, word):\n",
    "        '''\n",
    "        takes a single wrod and gets the corresponding tensor\n",
    "        '''\n",
    "        word_lst = get_words_from_sentence(remove_zh_punctuation(word))\n",
    "        indices = [zh_vocab[word] for word in word_lst]\n",
    "        # get tensor \n",
    "        return torch.tensor(indices, dtype=torch.long).to(self.device)\n",
    "    \n",
    "    def forward(self, hidden, sentence=None):\n",
    "        '''\n",
    "        does the forward propagation. If sentence is provided, then we do teacher-forcing. Else we assume it is inference  \n",
    "            Params:\n",
    "                hidden: the hidden state passed from the previous \n",
    "                sentence: a sentence to be used for teacher-forcing, as a tensor \n",
    "                Make sure the teacher-forcing sentence is sliced to not include the last token [:-1]\n",
    "        '''\n",
    "        if sentence is not None:\n",
    "            # use teacher forcing \n",
    "            embeds = self.embeddings(sentence)\n",
    "            embeds_reshaped = embeds.view(len(embeds), 1, -1)\n",
    "            out, hidden = self.LSTM(embeds_reshaped, hidden)\n",
    "            logits = self.linear(out)\n",
    "            return logits\n",
    "        else:\n",
    "            # inference can stay the same (we want one at a time to check if the out put is eos)\n",
    "            # '<SOS>' token \n",
    "            # just generate some tokens, starting from the <sos> token\n",
    "            start_token = self.word_to_tensor('<SOS>')\n",
    "            # run through embedding layer\n",
    "            prev_char = start_token\n",
    "            outputs = []\n",
    "            for i in range(MAX_RESPONSE_LENGTH):\n",
    "                embeds = self.embeddings(prev_char).to(self.device)\n",
    "                out, hidden = self.LSTM(embeds.view(1, 1, -1), hidden)\n",
    "                logits = self.linear(out)\n",
    "                outputs.append(logits)\n",
    "                pred_idx = torch.argmax(logits, dim=2).item()\n",
    "                prev_char = torch.tensor(pred_idx, dtype=torch.long, device=self.device)\n",
    "            return torch.cat(outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions to take a sentence and turn it into a tensor, adding <sos> and <eos>\n",
    "def sequence_to_tensor_en(sequence):\n",
    "    '''\n",
    "    takes sequence and converts to tensor \n",
    "    '''\n",
    "    # add \"<SOS> and <EOS>\"\n",
    "    words = get_words_from_sentence(\"<SOS> \" + remove_punctuation(sequence).lower() + \" <EOS>\")\n",
    "    \n",
    "    # convert to indices, reverting to <UNK> token\n",
    "    word_indices = [ en_vocab[word] if word in en_vocab else en_vocab[\"<UNK>\"] for word in words ]\n",
    "    return torch.tensor(word_indices, dtype=torch.long)\n",
    "    \n",
    "\n",
    "def sequence_to_tensor_zh(sequence):\n",
    "    '''\n",
    "    takes sequence and converts to chinese tensor \n",
    "    '''\n",
    "    words = ([\"<SOS>\"] + list(remove_zh_punctuation(sequence)))\n",
    "    words.append(\"<EOS>\")\n",
    "    \n",
    "    word_indices = [ zh_vocab[word] if word in zh_vocab else zh_vocab[\"<UNK>\"] for word in words ]\n",
    "    return torch.tensor(word_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a full run through both the Encoder and the decoder \n",
    "\n",
    "encoder = Encoder(embedding_dim=3, vocab_size=len(en_vocab), hidden_dim=5)\n",
    "en_sentence = \"I love bread.\"\n",
    "zh_sentence = \"我爱面包\"\n",
    "\n",
    "\n",
    "h_n, c_n = encoder.forward(sequence_to_tensor_en(en_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train  200000\n",
      "Length of zh dictionary  5613\n",
      "Length of english dictionary  31568\n"
     ]
    }
   ],
   "source": [
    "## new 100,000 len\n",
    "print(\"Length of train \", len(train_set_mini))\n",
    "print(\"Length of zh dictionary \", len(zh_vocab))\n",
    "print(\"Length of english dictionary \", len(en_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zh_tensor_outputs_to_sentence(output_tensor):\n",
    "    s = ''\n",
    "    zh_vocab_lst = list(zh_vocab.keys())\n",
    "    for word_tensor in output_tensor:\n",
    "        pred_idx = torch.argmax(word_tensor, dim=-1).item()\n",
    "        s += zh_vocab_lst[pred_idx]\n",
    "    return s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does mps save time?\n",
    "琥ä纷随哗良钮眼黎s\n",
    "Number of trains 1000\n",
    "Loss 6.893100890159607\n",
    "time for 1000 : mps: 13.119836807250977\n",
    "\n",
    "CPU:\n",
    "Number of trains 1000\n",
    "Loss 6.896958403587341\n",
    "time for 1000 : cpu: 23.142292022705078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "攀祂夺挲蒙痿梧舛阡栎\n",
      "Number of trains 1000\n",
      "Loss 6.890591035366058\n",
      "Number of trains 2000\n",
      "Loss 6.536398762226105\n",
      "Number of trains 3000\n",
      "Loss 6.311073744058609\n",
      "Number of trains 4000\n",
      "Loss 6.114545065164566\n",
      "Number of trains 5000\n",
      "Loss 6.013179780244827\n",
      "Number of trains 6000\n",
      "Loss 5.939472044944763\n",
      "Number of trains 7000\n",
      "Loss 5.800854883670807\n",
      "Number of trains 8000\n",
      "Loss 5.774311278581619\n",
      "Number of trains 9000\n",
      "Loss 5.654200109958649\n",
      "Number of trains 10000\n",
      "Loss 5.682991857051849\n",
      "Number of trains 11000\n",
      "Loss 5.632449702739716\n",
      "Number of trains 12000\n",
      "Loss 5.590144649028778\n",
      "Number of trains 13000\n",
      "Loss 5.54178799700737\n",
      "Number of trains 14000\n",
      "Loss 5.516321888923645\n",
      "Number of trains 15000\n",
      "Loss 5.480177397966385\n",
      "Number of trains 16000\n",
      "Loss 5.458978399753571\n",
      "Number of trains 17000\n",
      "Loss 5.36113609623909\n",
      "Number of trains 18000\n",
      "Loss 5.45415352511406\n",
      "Number of trains 19000\n",
      "Loss 5.412057629108429\n",
      "Number of trains 20000\n",
      "Loss 5.3729295177459715\n",
      "Number of trains 21000\n",
      "Loss 5.381785968780518\n",
      "Number of trains 22000\n",
      "Loss 5.29777060174942\n",
      "Number of trains 23000\n",
      "Loss 5.288871188402176\n",
      "Number of trains 24000\n",
      "Loss 5.301674452066422\n",
      "Number of trains 25000\n",
      "Loss 5.290337295293808\n",
      "Number of trains 26000\n",
      "Loss 5.250007460594177\n",
      "Number of trains 27000\n",
      "Loss 5.204783692836761\n",
      "Number of trains 28000\n",
      "Loss 5.228991359710693\n",
      "Number of trains 29000\n",
      "Loss 5.237305803537369\n",
      "Number of trains 30000\n",
      "Loss 5.221735704898834\n",
      "Number of trains 31000\n",
      "Loss 5.2005656456947325\n",
      "Number of trains 32000\n",
      "Loss 5.203262570381164\n",
      "Number of trains 33000\n",
      "Loss 5.189652161359787\n",
      "Number of trains 34000\n",
      "Loss 5.202770781993866\n",
      "Number of trains 35000\n",
      "Loss 5.207417265892029\n",
      "Number of trains 36000\n",
      "Loss 5.188396432876587\n",
      "Number of trains 37000\n",
      "Loss 5.177903192281723\n",
      "Number of trains 38000\n",
      "Loss 5.151720385789871\n",
      "Number of trains 39000\n",
      "Loss 5.118657212018967\n",
      "Number of trains 40000\n",
      "Loss 5.10210173535347\n",
      "Number of trains 41000\n",
      "Loss 5.1767795732021336\n",
      "Number of trains 42000\n",
      "Loss 5.108836319208145\n",
      "Number of trains 43000\n",
      "Loss 5.113540214061737\n",
      "Number of trains 44000\n",
      "Loss 5.1099661669731145\n",
      "Number of trains 45000\n",
      "Loss 5.0918358204364775\n",
      "Number of trains 46000\n",
      "Loss 5.077716681957245\n",
      "Number of trains 47000\n",
      "Loss 5.122415950775147\n",
      "Number of trains 48000\n",
      "Loss 5.084659039497375\n",
      "Number of trains 49000\n",
      "Loss 5.060252219676971\n",
      "Number of trains 50000\n",
      "Loss 5.055250511884689\n",
      "Number of trains 51000\n",
      "Loss 5.074187644004822\n",
      "Number of trains 52000\n",
      "Loss 5.08328390622139\n",
      "Number of trains 53000\n",
      "Loss 5.0710584511756895\n",
      "Number of trains 54000\n",
      "Loss 5.0214257752895355\n",
      "Number of trains 55000\n",
      "Loss 5.058997208118439\n",
      "Number of trains 56000\n",
      "Loss 5.017692456007004\n",
      "Number of trains 57000\n",
      "Loss 4.96357449054718\n",
      "Number of trains 58000\n",
      "Loss 5.035696056604386\n",
      "Number of trains 59000\n",
      "Loss 5.0394628338813785\n",
      "Number of trains 60000\n",
      "Loss 5.033755446910858\n",
      "Number of trains 61000\n",
      "Loss 4.984599766969681\n",
      "Number of trains 62000\n",
      "Loss 5.039028180837631\n",
      "Number of trains 63000\n",
      "Loss 5.02588586306572\n",
      "Number of trains 64000\n",
      "Loss 4.968039890050888\n",
      "Number of trains 65000\n",
      "Loss 4.968167759418487\n",
      "Number of trains 66000\n",
      "Loss 4.968956941366196\n",
      "Number of trains 67000\n",
      "Loss 5.027658346652984\n",
      "Number of trains 68000\n",
      "Loss 5.023997256040573\n",
      "Number of trains 69000\n",
      "Loss 4.983931601762771\n",
      "Number of trains 70000\n",
      "Loss 4.961819949150086\n",
      "Number of trains 71000\n",
      "Loss 4.971745314598084\n",
      "Number of trains 72000\n",
      "Loss 4.9713841810226445\n",
      "Number of trains 73000\n",
      "Loss 4.933616483449936\n",
      "Number of trains 74000\n",
      "Loss 4.968112955570221\n",
      "Number of trains 75000\n",
      "Loss 4.956134070158004\n",
      "Number of trains 76000\n",
      "Loss 4.985274991989136\n",
      "Number of trains 77000\n",
      "Loss 4.937930931806564\n",
      "Number of trains 78000\n",
      "Loss 4.945025078296661\n",
      "Number of trains 79000\n",
      "Loss 4.904876813173294\n",
      "Number of trains 80000\n",
      "Loss 4.944328748226166\n",
      "Number of trains 81000\n",
      "Loss 4.937778856992722\n",
      "Number of trains 82000\n",
      "Loss 4.910225883007049\n",
      "Number of trains 83000\n",
      "Loss 4.981048966884613\n",
      "Number of trains 84000\n",
      "Loss 4.952431351661682\n",
      "Number of trains 85000\n",
      "Loss 4.91382835483551\n",
      "Number of trains 86000\n",
      "Loss 4.921892384767532\n",
      "Number of trains 87000\n",
      "Loss 4.881488772153855\n",
      "Number of trains 88000\n",
      "Loss 4.916966042757034\n",
      "Number of trains 89000\n",
      "Loss 4.954490016698838\n",
      "Number of trains 90000\n",
      "Loss 4.976923570871353\n",
      "Number of trains 91000\n",
      "Loss 4.966572149038315\n",
      "Number of trains 92000\n",
      "Loss 4.899850131273269\n",
      "Number of trains 93000\n",
      "Loss 4.947486596822738\n",
      "Number of trains 94000\n",
      "Loss 4.881000511407852\n",
      "Number of trains 95000\n",
      "Loss 4.91224637389183\n",
      "Number of trains 96000\n",
      "Loss 4.895812995910645\n",
      "Number of trains 97000\n",
      "Loss 4.905667488336563\n",
      "Number of trains 98000\n",
      "Loss 4.883835131645203\n",
      "Number of trains 99000\n",
      "Loss 4.904649874687195\n",
      "Number of trains 100000\n",
      "Loss 4.921069054365158\n",
      "Number of trains 101000\n",
      "Loss 4.86419242310524\n",
      "Number of trains 102000\n",
      "Loss 4.923825612783432\n",
      "Number of trains 103000\n",
      "Loss 4.883184323072434\n",
      "Number of trains 104000\n",
      "Loss 4.889447315692902\n",
      "Number of trains 105000\n",
      "Loss 4.829576735496521\n",
      "Number of trains 106000\n",
      "Loss 4.899853232860565\n",
      "Number of trains 107000\n",
      "Loss 4.877930215835571\n",
      "Number of trains 108000\n",
      "Loss 4.873269285917282\n",
      "Number of trains 109000\n",
      "Loss 4.8932878837585445\n",
      "Number of trains 110000\n",
      "Loss 4.866021636009216\n",
      "Number of trains 111000\n",
      "Loss 4.891369935512543\n",
      "Number of trains 112000\n",
      "Loss 4.824060688495636\n",
      "Number of trains 113000\n",
      "Loss 4.895943136453629\n",
      "Number of trains 114000\n",
      "Loss 4.865912350654602\n",
      "Number of trains 115000\n",
      "Loss 4.84905455327034\n",
      "Number of trains 116000\n",
      "Loss 4.869783851146698\n",
      "Number of trains 117000\n",
      "Loss 4.862160022974014\n",
      "Number of trains 118000\n",
      "Loss 4.87817284989357\n",
      "Number of trains 119000\n",
      "Loss 4.844862372159958\n",
      "Number of trains 120000\n",
      "Loss 4.876106333017349\n",
      "Number of trains 121000\n",
      "Loss 4.892005294084549\n",
      "Number of trains 122000\n",
      "Loss 4.863085128307342\n",
      "Number of trains 123000\n",
      "Loss 4.822463633537293\n",
      "Number of trains 124000\n",
      "Loss 4.837159119844436\n",
      "Number of trains 125000\n",
      "Loss 4.85717918252945\n",
      "Number of trains 126000\n",
      "Loss 4.864579071044922\n",
      "Number of trains 127000\n",
      "Loss 4.79549154829979\n",
      "Number of trains 128000\n",
      "Loss 4.869289546251297\n",
      "Number of trains 129000\n",
      "Loss 4.821026487827301\n",
      "Number of trains 130000\n",
      "Loss 4.810694016695023\n",
      "Number of trains 131000\n",
      "Loss 4.851047726154327\n",
      "Number of trains 132000\n",
      "Loss 4.848040090799332\n",
      "Number of trains 133000\n",
      "Loss 4.828103847026825\n",
      "Number of trains 134000\n",
      "Loss 4.8311567559242246\n",
      "Number of trains 135000\n",
      "Loss 4.820414009571075\n",
      "Number of trains 136000\n",
      "Loss 4.839123223543167\n",
      "Number of trains 137000\n",
      "Loss 4.840232621431351\n",
      "Number of trains 138000\n",
      "Loss 4.793662244558335\n",
      "Number of trains 139000\n",
      "Loss 4.774634228229523\n",
      "Number of trains 140000\n",
      "Loss 4.861897478580475\n",
      "Number of trains 141000\n",
      "Loss 4.810151736736298\n",
      "Number of trains 142000\n",
      "Loss 4.8329854691028595\n",
      "Number of trains 143000\n",
      "Loss 4.814420928716659\n",
      "Number of trains 144000\n",
      "Loss 4.802094867706299\n",
      "Number of trains 145000\n",
      "Loss 4.779172473192215\n",
      "Number of trains 146000\n",
      "Loss 4.818974036216736\n",
      "Number of trains 147000\n",
      "Loss 4.825991287946701\n",
      "Number of trains 148000\n",
      "Loss 4.8053008563518524\n",
      "Number of trains 149000\n",
      "Loss 4.795049276351929\n",
      "Number of trains 150000\n",
      "Loss 4.860172247886657\n",
      "Number of trains 151000\n",
      "Loss 4.767000432014465\n",
      "Number of trains 152000\n",
      "Loss 4.825998616218567\n",
      "Number of trains 153000\n",
      "Loss 4.849579476833344\n",
      "Number of trains 154000\n",
      "Loss 4.865693692207336\n",
      "Number of trains 155000\n",
      "Loss 4.770609023809433\n",
      "Number of trains 156000\n",
      "Loss 4.831239892959594\n",
      "Number of trains 157000\n",
      "Loss 4.832164777755738\n",
      "Number of trains 158000\n",
      "Loss 4.787618385791778\n",
      "Number of trains 159000\n",
      "Loss 4.833985852479935\n",
      "Number of trains 160000\n",
      "Loss 4.770782986402511\n",
      "Number of trains 161000\n",
      "Loss 4.810615561008453\n",
      "Number of trains 162000\n",
      "Loss 4.795809210062027\n",
      "Number of trains 163000\n",
      "Loss 4.844078563928604\n",
      "Number of trains 164000\n",
      "Loss 4.791079199552536\n",
      "Number of trains 165000\n",
      "Loss 4.786845432519913\n",
      "Number of trains 166000\n",
      "Loss 4.820596848249435\n",
      "Number of trains 167000\n",
      "Loss 4.868528712987899\n",
      "Number of trains 168000\n",
      "Loss 4.81151524734497\n",
      "Number of trains 169000\n",
      "Loss 4.825684401035309\n",
      "Number of trains 170000\n",
      "Loss 4.803182439088822\n",
      "Number of trains 171000\n",
      "Loss 4.815766095638275\n",
      "Number of trains 172000\n",
      "Loss 4.790673691272736\n",
      "Number of trains 173000\n",
      "Loss 4.795050972700119\n",
      "Number of trains 174000\n",
      "Loss 4.800621820926667\n",
      "Number of trains 175000\n",
      "Loss 4.766573862314225\n",
      "Number of trains 176000\n",
      "Loss 4.826363015651703\n",
      "Number of trains 177000\n",
      "Loss 4.784202817440033\n",
      "Number of trains 178000\n",
      "Loss 4.807016355514526\n",
      "Number of trains 179000\n",
      "Loss 4.809894776582718\n",
      "Number of trains 180000\n",
      "Loss 4.833113031625747\n",
      "Number of trains 181000\n",
      "Loss 4.850997555971146\n",
      "Number of trains 182000\n",
      "Loss 4.779606147646904\n",
      "Number of trains 183000\n",
      "Loss 4.789667459726334\n",
      "Number of trains 184000\n",
      "Loss 4.804107578277588\n",
      "Number of trains 185000\n",
      "Loss 4.7759630246162414\n",
      "Number of trains 186000\n",
      "Loss 4.813251732826233\n",
      "Number of trains 187000\n",
      "Loss 4.815391219615936\n",
      "Number of trains 188000\n",
      "Loss 4.769491257190705\n",
      "Number of trains 189000\n",
      "Loss 4.704909193277359\n",
      "Number of trains 190000\n",
      "Loss 4.782555650234222\n",
      "Number of trains 191000\n",
      "Loss 4.7727010989189145\n",
      "Number of trains 192000\n",
      "Loss 4.774236178398132\n",
      "Number of trains 193000\n",
      "Loss 4.75592947268486\n",
      "Number of trains 194000\n",
      "Loss 4.7854618239402775\n",
      "Number of trains 195000\n",
      "Loss 4.790869194984436\n",
      "Number of trains 196000\n",
      "Loss 4.762021619319916\n",
      "Number of trains 197000\n",
      "Loss 4.778296820878983\n",
      "Number of trains 198000\n",
      "Loss 4.766735570907593\n",
      "Number of trains 199000\n",
      "Loss 4.774721089363098\n",
      "Number of trains 200000\n",
      "Loss 4.8038947005271915\n",
      "我的人<EOS><EOS><EOS><EOS><EOS><EOS><EOS>\n",
      "Number of trains 201000\n",
      "Loss 4.747265274047852\n",
      "Number of trains 202000\n",
      "Loss 4.782252739667893\n",
      "Number of trains 203000\n",
      "Loss 4.7798727376461025\n",
      "Number of trains 204000\n",
      "Loss 4.774769181251526\n",
      "Number of trains 205000\n",
      "Loss 4.797222980737686\n",
      "Number of trains 206000\n",
      "Loss 4.785864306688309\n",
      "Number of trains 207000\n",
      "Loss 4.739048021554947\n",
      "Number of trains 208000\n",
      "Loss 4.772077851295471\n",
      "Number of trains 209000\n",
      "Loss 4.712950637340546\n",
      "Number of trains 210000\n",
      "Loss 4.773967868804932\n",
      "Number of trains 211000\n",
      "Loss 4.783206141710282\n",
      "Number of trains 212000\n",
      "Loss 4.760453116655349\n",
      "Number of trains 213000\n",
      "Loss 4.73831748175621\n",
      "Number of trains 214000\n",
      "Loss 4.767412121772766\n",
      "Number of trains 215000\n",
      "Loss 4.751601286172867\n",
      "Number of trains 216000\n",
      "Loss 4.750494570255279\n",
      "Number of trains 217000\n",
      "Loss 4.6673109526634216\n",
      "Number of trains 218000\n",
      "Loss 4.798676308393478\n",
      "Number of trains 219000\n",
      "Loss 4.776429749011993\n",
      "Number of trains 220000\n",
      "Loss 4.741391424655914\n",
      "Number of trains 221000\n",
      "Loss 4.776913288354874\n",
      "Number of trains 222000\n",
      "Loss 4.700680027127266\n",
      "Number of trains 223000\n",
      "Loss 4.7104770548343655\n",
      "Number of trains 224000\n",
      "Loss 4.733914813756943\n",
      "Number of trains 225000\n",
      "Loss 4.737774221420288\n",
      "Number of trains 226000\n",
      "Loss 4.71346115899086\n",
      "Number of trains 227000\n",
      "Loss 4.684996510267258\n",
      "Number of trains 228000\n",
      "Loss 4.720471485614777\n",
      "Number of trains 229000\n",
      "Loss 4.741398119449616\n",
      "Number of trains 230000\n",
      "Loss 4.741755856752396\n",
      "Number of trains 231000\n",
      "Loss 4.727083516597748\n",
      "Number of trains 232000\n",
      "Loss 4.743169749259949\n",
      "Number of trains 233000\n",
      "Loss 4.724418681621551\n",
      "Number of trains 234000\n",
      "Loss 4.742420430421829\n",
      "Number of trains 235000\n",
      "Loss 4.756672083377838\n",
      "Number of trains 236000\n",
      "Loss 4.746433396339416\n",
      "Number of trains 237000\n",
      "Loss 4.749193632602692\n",
      "Number of trains 238000\n",
      "Loss 4.7392279679775235\n",
      "Number of trains 239000\n",
      "Loss 4.709450649261474\n",
      "Number of trains 240000\n",
      "Loss 4.697826174974441\n",
      "Number of trains 241000\n",
      "Loss 4.769115167617798\n",
      "Number of trains 242000\n",
      "Loss 4.723087270498276\n",
      "Number of trains 243000\n",
      "Loss 4.726474605321884\n",
      "Number of trains 244000\n",
      "Loss 4.728230303287506\n",
      "Number of trains 245000\n",
      "Loss 4.713864763975144\n",
      "Number of trains 246000\n",
      "Loss 4.7194597840309145\n",
      "Number of trains 247000\n",
      "Loss 4.7598135628700255\n",
      "Number of trains 248000\n",
      "Loss 4.735841776847839\n",
      "Number of trains 249000\n",
      "Loss 4.71445552277565\n",
      "Number of trains 250000\n",
      "Loss 4.701181654930115\n",
      "Number of trains 251000\n",
      "Loss 4.737901933193207\n",
      "Number of trains 252000\n",
      "Loss 4.740009991407394\n",
      "Number of trains 253000\n",
      "Loss 4.747775174856186\n",
      "Number of trains 254000\n",
      "Loss 4.70017152261734\n",
      "Number of trains 255000\n",
      "Loss 4.741052521228791\n",
      "Number of trains 256000\n",
      "Loss 4.692371857643128\n",
      "Number of trains 257000\n",
      "Loss 4.642058935403824\n",
      "Number of trains 258000\n",
      "Loss 4.717785887241363\n",
      "Number of trains 259000\n",
      "Loss 4.740370219945907\n",
      "Number of trains 260000\n",
      "Loss 4.726987218856811\n",
      "Number of trains 261000\n",
      "Loss 4.681570605754852\n",
      "Number of trains 262000\n",
      "Loss 4.741317067623139\n",
      "Number of trains 263000\n",
      "Loss 4.724465669155121\n",
      "Number of trains 264000\n",
      "Loss 4.678267432451248\n",
      "Number of trains 265000\n",
      "Loss 4.676993769645691\n",
      "Number of trains 266000\n",
      "Loss 4.690385077953339\n",
      "Number of trains 267000\n",
      "Loss 4.744412707328796\n",
      "Number of trains 268000\n",
      "Loss 4.757613901853562\n",
      "Number of trains 269000\n",
      "Loss 4.708532111406326\n",
      "Number of trains 270000\n",
      "Loss 4.691590488433838\n",
      "Number of trains 271000\n",
      "Loss 4.694826725721359\n",
      "Number of trains 272000\n",
      "Loss 4.707509893417359\n",
      "Number of trains 273000\n",
      "Loss 4.670568995714188\n",
      "Number of trains 274000\n",
      "Loss 4.713073833465576\n",
      "Number of trains 275000\n",
      "Loss 4.70206769323349\n",
      "Number of trains 276000\n",
      "Loss 4.723950624704361\n",
      "Number of trains 277000\n",
      "Loss 4.6762308628559115\n",
      "Number of trains 278000\n",
      "Loss 4.697149555325508\n",
      "Number of trains 279000\n",
      "Loss 4.648404590845108\n",
      "Number of trains 280000\n",
      "Loss 4.704106021642685\n",
      "Number of trains 281000\n",
      "Loss 4.689299023628235\n",
      "Number of trains 282000\n",
      "Loss 4.670762515544891\n",
      "Number of trains 283000\n",
      "Loss 4.744739512681961\n",
      "Number of trains 284000\n",
      "Loss 4.7090140812397\n",
      "Number of trains 285000\n",
      "Loss 4.679759428620338\n",
      "Number of trains 286000\n",
      "Loss 4.690619867324829\n",
      "Number of trains 287000\n",
      "Loss 4.644293714761734\n",
      "Number of trains 288000\n",
      "Loss 4.687962305545807\n",
      "Number of trains 289000\n",
      "Loss 4.728558084249497\n",
      "Number of trains 290000\n",
      "Loss 4.745041002511978\n",
      "Number of trains 291000\n",
      "Loss 4.747006966590881\n",
      "Number of trains 292000\n",
      "Loss 4.682932586669922\n",
      "Number of trains 293000\n",
      "Loss 4.730272481679917\n",
      "Number of trains 294000\n",
      "Loss 4.6669231195449825\n",
      "Number of trains 295000\n",
      "Loss 4.697861014604569\n",
      "Number of trains 296000\n",
      "Loss 4.6828333332538605\n",
      "Number of trains 297000\n",
      "Loss 4.695873552322388\n",
      "Number of trains 298000\n",
      "Loss 4.67225488448143\n",
      "Number of trains 299000\n",
      "Loss 4.6975159791708\n",
      "Number of trains 300000\n",
      "Loss 4.723630027055741\n",
      "Number of trains 301000\n",
      "Loss 4.660421095132827\n",
      "Number of trains 302000\n",
      "Loss 4.722865924596786\n",
      "Number of trains 303000\n",
      "Loss 4.685032914161682\n",
      "Number of trains 304000\n",
      "Loss 4.690496726512909\n",
      "Number of trains 305000\n",
      "Loss 4.639971956729889\n",
      "Number of trains 306000\n",
      "Loss 4.712143500804901\n",
      "Number of trains 307000\n",
      "Loss 4.686918973207474\n",
      "Number of trains 308000\n",
      "Loss 4.677056522607804\n",
      "Number of trains 309000\n",
      "Loss 4.708310297966003\n",
      "Number of trains 310000\n",
      "Loss 4.6778143837451935\n",
      "Number of trains 311000\n",
      "Loss 4.712560433745384\n",
      "Number of trains 312000\n",
      "Loss 4.626559010982513\n",
      "Number of trains 313000\n",
      "Loss 4.717032750844956\n",
      "Number of trains 314000\n",
      "Loss 4.6826862261295314\n",
      "Number of trains 315000\n",
      "Loss 4.66541953587532\n",
      "Number of trains 316000\n",
      "Loss 4.695517949581146\n",
      "Number of trains 317000\n",
      "Loss 4.694261602640152\n",
      "Number of trains 318000\n",
      "Loss 4.697387707471847\n",
      "Number of trains 319000\n",
      "Loss 4.670617531776428\n",
      "Number of trains 320000\n",
      "Loss 4.701792999982834\n",
      "Number of trains 321000\n",
      "Loss 4.721027059555054\n",
      "Number of trains 322000\n",
      "Loss 4.698802564382553\n",
      "Number of trains 323000\n",
      "Loss 4.655221090316773\n",
      "Number of trains 324000\n",
      "Loss 4.652475506782531\n",
      "Number of trains 325000\n",
      "Loss 4.683694005012512\n",
      "Number of trains 326000\n",
      "Loss 4.6947980637550355\n",
      "Number of trains 327000\n",
      "Loss 4.6290835287570955\n",
      "Number of trains 328000\n",
      "Loss 4.70970838880539\n",
      "Number of trains 329000\n",
      "Loss 4.659402218818665\n",
      "Number of trains 330000\n",
      "Loss 4.641434805631637\n",
      "Number of trains 331000\n",
      "Loss 4.690461579322815\n",
      "Number of trains 332000\n",
      "Loss 4.6762623236179355\n",
      "Number of trains 333000\n",
      "Loss 4.678535746335983\n",
      "Number of trains 334000\n",
      "Loss 4.666363281965256\n",
      "Number of trains 335000\n",
      "Loss 4.667603462696076\n",
      "Number of trains 336000\n",
      "Loss 4.678844563007354\n",
      "Number of trains 337000\n",
      "Loss 4.683392903566361\n",
      "Number of trains 338000\n",
      "Loss 4.6388689377307895\n",
      "Number of trains 339000\n",
      "Loss 4.624243322849273\n",
      "Number of trains 340000\n",
      "Loss 4.709499542951584\n",
      "Number of trains 341000\n",
      "Loss 4.65038304901123\n",
      "Number of trains 342000\n",
      "Loss 4.674040948152542\n",
      "Number of trains 343000\n",
      "Loss 4.662338935375214\n",
      "Number of trains 344000\n",
      "Loss 4.652048770666123\n",
      "Number of trains 345000\n",
      "Loss 4.632984952688217\n",
      "Number of trains 346000\n",
      "Loss 4.668633255243301\n",
      "Number of trains 347000\n",
      "Loss 4.669113547325134\n",
      "Number of trains 348000\n",
      "Loss 4.657732274770737\n",
      "Number of trains 349000\n",
      "Loss 4.644733807802201\n",
      "Number of trains 350000\n",
      "Loss 4.705511924743653\n",
      "Number of trains 351000\n",
      "Loss 4.6310860567092895\n",
      "Number of trains 352000\n",
      "Loss 4.68360707449913\n",
      "Number of trains 353000\n",
      "Loss 4.704758324623108\n",
      "Number of trains 354000\n",
      "Loss 4.723536309480667\n",
      "Number of trains 355000\n",
      "Loss 4.623373541593551\n",
      "Number of trains 356000\n",
      "Loss 4.691737948179245\n",
      "Number of trains 357000\n",
      "Loss 4.695963297843933\n",
      "Number of trains 358000\n",
      "Loss 4.644218381881714\n",
      "Number of trains 359000\n",
      "Loss 4.688508103132248\n",
      "Number of trains 360000\n",
      "Loss 4.638712526082992\n",
      "Number of trains 361000\n",
      "Loss 4.675002099037171\n",
      "Number of trains 362000\n",
      "Loss 4.66271078133583\n",
      "Number of trains 363000\n",
      "Loss 4.702685303926468\n",
      "Number of trains 364000\n",
      "Loss 4.65955672287941\n",
      "Number of trains 365000\n",
      "Loss 4.657027089595795\n",
      "Number of trains 366000\n",
      "Loss 4.682712054729461\n",
      "Number of trains 367000\n",
      "Loss 4.730145057439804\n",
      "Number of trains 368000\n",
      "Loss 4.6729637911319735\n",
      "Number of trains 369000\n",
      "Loss 4.687798831224441\n",
      "Number of trains 370000\n",
      "Loss 4.674610659360885\n",
      "Number of trains 371000\n",
      "Loss 4.692592855215072\n",
      "Number of trains 372000\n",
      "Loss 4.665133923530578\n",
      "Number of trains 373000\n",
      "Loss 4.66277118062973\n",
      "Number of trains 374000\n",
      "Loss 4.670409883260727\n",
      "Number of trains 375000\n",
      "Loss 4.639281217813492\n",
      "Number of trains 376000\n",
      "Loss 4.693882053375244\n",
      "Number of trains 377000\n",
      "Loss 4.655986392498017\n",
      "Number of trains 378000\n",
      "Loss 4.674516964435577\n",
      "Number of trains 379000\n",
      "Loss 4.680714194536209\n",
      "Number of trains 380000\n",
      "Loss 4.696693759202957\n",
      "Number of trains 381000\n",
      "Loss 4.72464918923378\n",
      "Number of trains 382000\n",
      "Loss 4.65920797431469\n",
      "Number of trains 383000\n",
      "Loss 4.663317426681519\n",
      "Number of trains 384000\n",
      "Loss 4.674476327419281\n",
      "Number of trains 385000\n",
      "Loss 4.6592155315876\n",
      "Number of trains 386000\n",
      "Loss 4.688819981336594\n",
      "Number of trains 387000\n",
      "Loss 4.691294350147247\n",
      "Number of trains 388000\n",
      "Loss 4.6469304378032685\n",
      "Number of trains 389000\n",
      "Loss 4.583118262290955\n",
      "Number of trains 390000\n",
      "Loss 4.663705262184143\n",
      "Number of trains 391000\n",
      "Loss 4.658938630580902\n",
      "Number of trains 392000\n",
      "Loss 4.661868939161301\n",
      "Number of trains 393000\n",
      "Loss 4.63278129029274\n",
      "Number of trains 394000\n",
      "Loss 4.672422701597214\n",
      "Number of trains 395000\n",
      "Loss 4.676002153635025\n",
      "Number of trains 396000\n",
      "Loss 4.639695614814758\n",
      "Number of trains 397000\n",
      "Loss 4.6597931767702105\n",
      "Number of trains 398000\n",
      "Loss 4.655367770195007\n",
      "Number of trains 399000\n",
      "Loss 4.660911693572998\n",
      "Number of trains 400000\n",
      "Loss 4.685890111923218\n",
      "我的朋友<EOS><EOS><EOS><EOS><EOS><EOS>\n",
      "Number of trains 401000\n",
      "Loss 4.640237184286118\n",
      "Number of trains 402000\n",
      "Loss 4.663340981245041\n",
      "Number of trains 403000\n",
      "Loss 4.6627278425693515\n",
      "Number of trains 404000\n",
      "Loss 4.665258610010147\n",
      "Number of trains 405000\n",
      "Loss 4.689700705289841\n",
      "Number of trains 406000\n",
      "Loss 4.662536044120788\n",
      "Number of trains 407000\n",
      "Loss 4.6278370473384856\n",
      "Number of trains 408000\n",
      "Loss 4.6608687610626225\n",
      "Number of trains 409000\n",
      "Loss 4.605109849691391\n",
      "Number of trains 410000\n",
      "Loss 4.667896841287613\n",
      "Number of trains 411000\n",
      "Loss 4.67335515666008\n",
      "Number of trains 412000\n",
      "Loss 4.659797540426254\n",
      "Number of trains 413000\n",
      "Loss 4.630246207714081\n",
      "Number of trains 414000\n",
      "Loss 4.663176552772522\n",
      "Number of trains 415000\n",
      "Loss 4.644800629854203\n",
      "Number of trains 416000\n",
      "Loss 4.6389338676929475\n",
      "Number of trains 417000\n",
      "Loss 4.558073168754578\n",
      "Number of trains 418000\n",
      "Loss 4.6970364663600925\n",
      "Number of trains 419000\n",
      "Loss 4.669578360080719\n",
      "Number of trains 420000\n",
      "Loss 4.638084852933884\n",
      "Number of trains 421000\n",
      "Loss 4.67137140917778\n",
      "Number of trains 422000\n",
      "Loss 4.602126818537712\n",
      "Number of trains 423000\n",
      "Loss 4.612250977277756\n",
      "Number of trains 424000\n",
      "Loss 4.633443246603012\n",
      "Number of trains 425000\n",
      "Loss 4.645193974733353\n",
      "Number of trains 426000\n",
      "Loss 4.614339524269104\n",
      "Number of trains 427000\n",
      "Loss 4.590085151910782\n",
      "Number of trains 428000\n",
      "Loss 4.621610319137573\n",
      "Number of trains 429000\n",
      "Loss 4.6403512296676634\n",
      "Number of trains 430000\n",
      "Loss 4.636499498605728\n",
      "Number of trains 431000\n",
      "Loss 4.632751492738723\n",
      "Number of trains 432000\n",
      "Loss 4.649822456598282\n",
      "Number of trains 433000\n",
      "Loss 4.629795883655548\n",
      "Number of trains 434000\n",
      "Loss 4.643821665763855\n",
      "Number of trains 435000\n",
      "Loss 4.663060734987259\n",
      "Number of trains 436000\n",
      "Loss 4.651075757980347\n",
      "Number of trains 437000\n",
      "Loss 4.657597592353821\n",
      "Number of trains 438000\n",
      "Loss 4.642629273414612\n",
      "Number of trains 439000\n",
      "Loss 4.61720544219017\n",
      "Number of trains 440000\n",
      "Loss 4.606010645866394\n",
      "Number of trains 441000\n",
      "Loss 4.675385581970215\n",
      "Number of trains 442000\n",
      "Loss 4.622969558954239\n",
      "Number of trains 443000\n",
      "Loss 4.631296886444092\n",
      "Number of trains 444000\n",
      "Loss 4.644609522104263\n",
      "Number of trains 445000\n",
      "Loss 4.627731805801392\n",
      "Number of trains 446000\n",
      "Loss 4.620665485858917\n",
      "Number of trains 447000\n",
      "Loss 4.6752708673477175\n",
      "Number of trains 448000\n",
      "Loss 4.639591843605041\n",
      "Number of trains 449000\n",
      "Loss 4.623737285852433\n",
      "Number of trains 450000\n",
      "Loss 4.610923208475113\n",
      "Number of trains 451000\n",
      "Loss 4.649894300460815\n",
      "Number of trains 452000\n",
      "Loss 4.655401124715805\n",
      "Number of trains 453000\n",
      "Loss 4.660759286880493\n",
      "Number of trains 454000\n",
      "Loss 4.612134503364563\n",
      "Number of trains 455000\n",
      "Loss 4.645350783348084\n",
      "Number of trains 456000\n",
      "Loss 4.604471173763275\n",
      "Number of trains 457000\n",
      "Loss 4.562765618443489\n",
      "Number of trains 458000\n",
      "Loss 4.628627474308014\n",
      "Number of trains 459000\n",
      "Loss 4.652535886764526\n",
      "Number of trains 460000\n",
      "Loss 4.63913830113411\n",
      "Number of trains 461000\n",
      "Loss 4.599760172605515\n",
      "Number of trains 462000\n",
      "Loss 4.648355332374573\n",
      "Number of trains 463000\n",
      "Loss 4.642299019098282\n",
      "Number of trains 464000\n",
      "Loss 4.584292352080345\n",
      "Number of trains 465000\n",
      "Loss 4.588138046741485\n",
      "Number of trains 466000\n",
      "Loss 4.606724796772003\n",
      "Number of trains 467000\n",
      "Loss 4.6619493236541745\n",
      "Number of trains 468000\n",
      "Loss 4.675086593866348\n",
      "Number of trains 469000\n",
      "Loss 4.622206495285035\n",
      "Number of trains 470000\n",
      "Loss 4.604106021642685\n",
      "Number of trains 471000\n",
      "Loss 4.6128447576761245\n",
      "Number of trains 472000\n",
      "Loss 4.629436992645264\n",
      "Number of trains 473000\n",
      "Loss 4.591426102638245\n",
      "Number of trains 474000\n",
      "Loss 4.634620017766952\n",
      "Number of trains 475000\n",
      "Loss 4.6156155128479\n",
      "Number of trains 476000\n",
      "Loss 4.639973331212998\n",
      "Number of trains 477000\n",
      "Loss 4.5912433800697325\n",
      "Number of trains 478000\n",
      "Loss 4.613161528348923\n",
      "Number of trains 479000\n",
      "Loss 4.566409043550491\n",
      "Number of trains 480000\n",
      "Loss 4.62328301858902\n",
      "Number of trains 481000\n",
      "Loss 4.605932697296143\n",
      "Number of trains 482000\n",
      "Loss 4.59113439321518\n",
      "Number of trains 483000\n",
      "Loss 4.671874396562576\n",
      "Number of trains 484000\n",
      "Loss 4.6199314587116245\n",
      "Number of trains 485000\n",
      "Loss 4.608077486872673\n",
      "Number of trains 486000\n",
      "Loss 4.620593980312347\n",
      "Number of trains 487000\n",
      "Loss 4.575608534812927\n",
      "Number of trains 488000\n",
      "Loss 4.606750791311264\n",
      "Number of trains 489000\n",
      "Loss 4.654883508205414\n",
      "Number of trains 490000\n",
      "Loss 4.661112272739411\n",
      "Number of trains 491000\n",
      "Loss 4.672761612176895\n",
      "Number of trains 492000\n",
      "Loss 4.602513028621674\n",
      "Number of trains 493000\n",
      "Loss 4.651814868688583\n",
      "Number of trains 494000\n",
      "Loss 4.582113368034363\n",
      "Number of trains 495000\n",
      "Loss 4.62382799744606\n",
      "Number of trains 496000\n",
      "Loss 4.607401551246643\n",
      "Number of trains 497000\n",
      "Loss 4.614247204780579\n",
      "Number of trains 498000\n",
      "Loss 4.596309927225113\n",
      "Number of trains 499000\n",
      "Loss 4.6209574184417725\n",
      "Number of trains 500000\n",
      "Loss 4.6517953817844395\n",
      "Number of trains 501000\n",
      "Loss 4.580431599140168\n",
      "Number of trains 502000\n",
      "Loss 4.64707435464859\n",
      "Number of trains 503000\n",
      "Loss 4.610250553607941\n",
      "Number of trains 504000\n",
      "Loss 4.616800330400467\n",
      "Number of trains 505000\n",
      "Loss 4.553624648571015\n",
      "Number of trains 506000\n",
      "Loss 4.630008957386017\n",
      "Number of trains 507000\n",
      "Loss 4.606542922019958\n",
      "Number of trains 508000\n",
      "Loss 4.5978562276363375\n",
      "Number of trains 509000\n",
      "Loss 4.636324568986892\n",
      "Number of trains 510000\n",
      "Loss 4.600684595108032\n",
      "Number of trains 511000\n",
      "Loss 4.63511312019825\n",
      "Number of trains 512000\n",
      "Loss 4.557027883052826\n",
      "Number of trains 513000\n",
      "Loss 4.638749821662903\n",
      "Number of trains 514000\n",
      "Loss 4.610370542049408\n",
      "Number of trains 515000\n",
      "Loss 4.587633201360703\n",
      "Number of trains 516000\n",
      "Loss 4.620595312833786\n",
      "Number of trains 517000\n",
      "Loss 4.613131978273392\n",
      "Number of trains 518000\n",
      "Loss 4.619067741394043\n",
      "Number of trains 519000\n",
      "Loss 4.604597034931182\n",
      "Number of trains 520000\n",
      "Loss 4.626128995656967\n",
      "Number of trains 521000\n",
      "Loss 4.64862016248703\n",
      "Number of trains 522000\n",
      "Loss 4.618734879255295\n",
      "Number of trains 523000\n",
      "Loss 4.5815466339588165\n",
      "Number of trains 524000\n",
      "Loss 4.583118446111679\n",
      "Number of trains 525000\n",
      "Loss 4.610467353105545\n",
      "Number of trains 526000\n",
      "Loss 4.627504281282425\n",
      "Number of trains 527000\n",
      "Loss 4.555723828792572\n",
      "Number of trains 528000\n",
      "Loss 4.6257753084898\n",
      "Number of trains 529000\n",
      "Loss 4.578799735546112\n",
      "Number of trains 530000\n",
      "Loss 4.576632047653198\n",
      "Number of trains 531000\n",
      "Loss 4.625276733636856\n",
      "Number of trains 532000\n",
      "Loss 4.614446262359619\n",
      "Number of trains 533000\n",
      "Loss 4.603721335887909\n",
      "Number of trains 534000\n",
      "Loss 4.5961622009277345\n",
      "Number of trains 535000\n",
      "Loss 4.593752376556396\n",
      "Number of trains 536000\n",
      "Loss 4.610023087501526\n",
      "Number of trains 537000\n",
      "Loss 4.620529270172119\n",
      "Number of trains 538000\n",
      "Loss 4.573630304336548\n",
      "Number of trains 539000\n",
      "Loss 4.556025902986526\n",
      "Number of trains 540000\n",
      "Loss 4.64496034526825\n",
      "Number of trains 541000\n",
      "Loss 4.576397211551666\n",
      "Number of trains 542000\n",
      "Loss 4.6063857791423795\n",
      "Number of trains 543000\n",
      "Loss 4.596043083667755\n",
      "Number of trains 544000\n",
      "Loss 4.586836062431336\n",
      "Number of trains 545000\n",
      "Loss 4.569642158985138\n",
      "Number of trains 546000\n",
      "Loss 4.592449689865112\n",
      "Number of trains 547000\n",
      "Loss 4.610269186019898\n",
      "Number of trains 548000\n",
      "Loss 4.589415072917938\n",
      "Number of trains 549000\n",
      "Loss 4.579744823455811\n",
      "Number of trains 550000\n",
      "Loss 4.640301290750504\n",
      "Number of trains 551000\n",
      "Loss 4.567232707977295\n",
      "Number of trains 552000\n",
      "Loss 4.618520795106888\n",
      "Number of trains 553000\n",
      "Loss 4.639704641103744\n",
      "Number of trains 554000\n",
      "Loss 4.661109237909317\n",
      "Number of trains 555000\n",
      "Loss 4.562267033100128\n",
      "Number of trains 556000\n",
      "Loss 4.62638750243187\n",
      "Number of trains 557000\n",
      "Loss 4.625456384181977\n",
      "Number of trains 558000\n",
      "Loss 4.580293071985245\n",
      "Number of trains 559000\n",
      "Loss 4.62221883058548\n",
      "Number of trains 560000\n",
      "Loss 4.5886731441020965\n",
      "Number of trains 561000\n",
      "Loss 4.6072044820785525\n",
      "Number of trains 562000\n",
      "Loss 4.5994934179782865\n",
      "Number of trains 563000\n",
      "Loss 4.6448010420799255\n",
      "Number of trains 564000\n",
      "Loss 4.5961346406936645\n",
      "Number of trains 565000\n",
      "Loss 4.5952801580429075\n",
      "Number of trains 566000\n",
      "Loss 4.620393403291702\n",
      "Number of trains 567000\n",
      "Loss 4.6669353716373445\n",
      "Number of trains 568000\n",
      "Loss 4.613939524650574\n",
      "Number of trains 569000\n",
      "Loss 4.620196079730987\n",
      "Number of trains 570000\n",
      "Loss 4.617654651165009\n",
      "Number of trains 571000\n",
      "Loss 4.62859609413147\n",
      "Number of trains 572000\n",
      "Loss 4.606858614683151\n",
      "Number of trains 573000\n",
      "Loss 4.607191769361496\n",
      "Number of trains 574000\n",
      "Loss 4.611218324422836\n",
      "Number of trains 575000\n",
      "Loss 4.581210010528564\n",
      "Number of trains 576000\n",
      "Loss 4.630591133356094\n",
      "Number of trains 577000\n",
      "Loss 4.598818358182907\n",
      "Number of trains 578000\n",
      "Loss 4.618209378242493\n",
      "Number of trains 579000\n",
      "Loss 4.61514947772026\n",
      "Number of trains 580000\n",
      "Loss 4.632219453573227\n",
      "Number of trains 581000\n",
      "Loss 4.668247948169708\n",
      "Number of trains 582000\n",
      "Loss 4.5950708957910535\n",
      "Number of trains 583000\n",
      "Loss 4.6015496952533725\n",
      "Number of trains 584000\n",
      "Loss 4.619696815252304\n",
      "Number of trains 585000\n",
      "Loss 4.5992913023233415\n",
      "Number of trains 586000\n",
      "Loss 4.624798066854477\n",
      "Number of trains 587000\n",
      "Loss 4.637849744558334\n",
      "Number of trains 588000\n",
      "Loss 4.589481461048126\n",
      "Number of trains 589000\n",
      "Loss 4.52739822602272\n",
      "Number of trains 590000\n",
      "Loss 4.608781953573227\n",
      "Number of trains 591000\n",
      "Loss 4.599491381645203\n",
      "Number of trains 592000\n",
      "Loss 4.610791407346725\n",
      "Number of trains 593000\n",
      "Loss 4.576422610521316\n",
      "Number of trains 594000\n",
      "Loss 4.621082188129425\n",
      "Number of trains 595000\n",
      "Loss 4.63115090572834\n",
      "Number of trains 596000\n",
      "Loss 4.582565812349319\n",
      "Number of trains 597000\n",
      "Loss 4.602107791304588\n",
      "Number of trains 598000\n",
      "Loss 4.595869371175766\n",
      "Number of trains 599000\n",
      "Loss 4.605812393665314\n",
      "Number of trains 600000\n",
      "Loss 4.634232846498489\n",
      "我喜欢我的<EOS>水<EOS><EOS><EOS>\n",
      "Total training time: 7301.97625207901\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train(num_epochs, training_data, encoder, decoder, device, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()), lr=lr\n",
    ")\n",
    "    count = 0\n",
    "    total_loss = 0\n",
    "    # see random prediction\n",
    "    predict_en_sequence = \"I love bread\" \n",
    "    predict_en_tensor = sequence_to_tensor_en(predict_en_sequence).to(device)\n",
    "    predict_hidden = encoder(predict_en_tensor)\n",
    "    print(zh_tensor_outputs_to_sentence(decoder.forward(predict_hidden)))\n",
    "    start_time = time.time()\n",
    "    for i in range(num_epochs):\n",
    "        for pair in training_data:\n",
    "            count += 1\n",
    "            if count % 1000 == 0:\n",
    "                print(f\"Number of trains {count}\")\n",
    "                # print the loss \n",
    "                print(f\"Loss {total_loss / 1000}\")\n",
    "                total_loss = 0\n",
    "            english = pair['english']\n",
    "            zh = pair['chinese']\n",
    "            en_tensor = sequence_to_tensor_en(english)\n",
    "            zh_tensor = sequence_to_tensor_zh(zh)\n",
    "            # pass to device \n",
    "            en_tensor = sequence_to_tensor_en(english).to(device)\n",
    "            zh_tensor = sequence_to_tensor_zh(zh).to(device)\n",
    "\n",
    "            h_n, c_n = encoder.forward(en_tensor)\n",
    "            target = zh_tensor[1:]\n",
    "            predicted = decoder.forward((h_n, c_n), zh_tensor[:-1])\n",
    "            \n",
    "            loss = nn.functional.cross_entropy(torch.squeeze(predicted), target)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        predict_hidden = encoder(predict_en_tensor)\n",
    "        print(zh_tensor_outputs_to_sentence(decoder.forward(predict_hidden)))\n",
    "    print(f\"Total training time: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "encoder = Encoder(embedding_dim=32, vocab_size=len(en_vocab), hidden_dim=128)\n",
    "decoder = Decoder(embedding_dim=32, vocab_size=len(zh_vocab), hidden_dim=128, device=device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "train(3, train_set_mini, encoder, decoder, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save encoder and decoder \n",
    "torch.save(encoder.state_dict(), './trained_models/baseline_encoder.pth')\n",
    "torch.save(decoder.state_dict(), './trained_models/baseline_decoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load previous model \n",
    "with open('vocab/en_vocab.pkl', 'rb') as f:\n",
    "    en_vocab = pickle.load(f)\n",
    "\n",
    "with open('vocab/zh_vocab.pkl', 'rb') as f:\n",
    "    zh_vocab = pickle.load(f)\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "encoder = Encoder(embedding_dim=32, vocab_size=len(en_vocab), hidden_dim=128)\n",
    "decoder = Decoder(embedding_dim=32, vocab_size=len(zh_vocab), hidden_dim=128, device=device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "encoder.load_state_dict(torch.load('./trained_models/baseline_encoder.pth'))\n",
    "decoder.load_state_dict(torch.load('./trained_models/baseline_decoder.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你的话你是什么<EOS>吗<EOS>\n"
     ]
    }
   ],
   "source": [
    "def translate_en_to_zh(encoder, decoder, sequence):\n",
    "        hidden = encoder.forward(sequence_to_tensor_en(sequence).to(device=\"mps\"))\n",
    "        print(zh_tensor_outputs_to_sentence(decoder.forward(hidden)))\n",
    "\n",
    "translate_en_to_zh(encoder, decoder, \"What is your name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTENTION MECHANISM\n",
    "- we will now work on adding the attention mechanism\n",
    "- we will try to add Luong attention \n",
    "- current reading source:   https://arxiv.org/pdf/1508.04025\n",
    "\n",
    "### Luong attention in the model\n",
    "- modfies the decoder\n",
    "- at each decoding step, instead of relying only on the previous hidden state, the decoder now looks back at all encoder outputs and chooses which parts to pay attention to. \n",
    "\n",
    "Key ideas:\n",
    "    - Encoder still processes the whole input and outputs as sequence of hidden states:\n",
    "\n",
    "    - Decoder at time step t has its own hidden state st.\n",
    "    - Attention score is computed between st (current decoder hidden state) and each hi (encoder hidden state)\n",
    "    - scoring function (how you actually calculate the attention score )\n",
    "        - dot product score(st, hi) = st * Wa * hi \n",
    "    \n",
    "    - for all encoder outputs, softmax the scores to get a probability distribution over input positions  (adds up to 1)\n",
    "\n",
    "    - context vector:\n",
    "        - weighted sum of encoder hidden states using the attention weights \n",
    "        - a \"summary\" of the input that the decoder should focus on at step t \n",
    "    \n",
    "    final step: combine the context vector, and the decoder hidden state to predict the next word. \n",
    "\n",
    "- decoder hidden --> compute scores with all encoder outputs --> softmax --> get context --> combine with decoder hidden --> predict word \n",
    "\n",
    "\n",
    "### score\n",
    "- we calculate a score vector\n",
    "- this score vector is softmaxed, this turns it into a probability vector (adds up to 1)\n",
    "- whatever we multiply with this softmaxed score vector has the effect of being amplified or ignored. If the corresponding attention cell is close to 1, it is amplified, but if it is small, the value goes to 0\n",
    "\n",
    "- we are going to use Luong Attention general version \n",
    "\n",
    "$$\n",
    "score = h_t^{T} W_a h_s\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preimplementation thoughts\n",
    "\n",
    "1. Encoder stays the same. we just keep track of out, as it represents the entire hidden state across all time steps.\n",
    "\n",
    "out, hidden = Encoder(sequence)\n",
    "\n",
    "2. We change the teacher forcing code to be one time step at a time (not vectorized, going to map it exactly as the inference logic. \n",
    "\n",
    "Whatever changes I mention down here will apply to both the teacher forcing and inference blocks:\n",
    "\n",
    "3. run:\n",
    "out, hidden = self.LSTM(embeds.view(1, 1, -1), hidden)\n",
    "\n",
    "to give us the hidden state for this current time step.\n",
    "\n",
    "3. With encoder out, representing all hidden states, and hidden representing the decoder hidden state for this current time step, we have all we need to create the SCORES tensor.\n",
    "\ta. We do this by passing in OUT to a NEW linear layer (I still need to figure out the dimensions, but I'll just do some math during the implementation time). This is dot with decoder hidden . I know it should be out, because we want to use the ALL hidden states, that's the point of attention. \n",
    "\n",
    "    b. Now we will have a scores tensor. I softmax this so as to create a vector that can amplify / minimize whatever its multiplied by. I multiply this softmax vector with out (all encoder hidden states). This gives a weighted out tensor, I sum it up to give us a single context tensor.\n",
    "\n",
    "4. with the context tensor, we concatenate with CURRENT decoder hidden state torch.cat(context, hidden)\n",
    "\n",
    "5. We pass this combined tensor to the already existing linear layer. By already existing linear layer, i mean the second line of this code where we get the logits\n",
    "\n",
    "                out, hidden = self.LSTM(embeds.view(1, 1, -1), hidden)\n",
    "                logits = self.linear(out) # this will be self.linear(combined_tensor)\n",
    "\n",
    "6. everything else stays the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training runs\n",
    "1. 04/25/25: first successful training run, 2 epochs @ 100k examples each. Model outputs non-garbage. Recognizes pronouns, and uses EOS tag. \n",
    "2. 04/25/25: Added vectorization to teacher forcing forward code. Use MPS, 3 epochs @200k examples each. Loss drops to around 4.6. Model still learns pronouns, but also common words like \"need\" --> 想要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
