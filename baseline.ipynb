{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq model for English to Chinese translation (baseline model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import string\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing, loading, and cleaning dataset\n",
    "\n",
    "- going to use datasets from: https://www.kaggle.com/datasets/qianhuan/translation?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5161434\n",
      "{'english': 'For greater sharpness, but with a slight increase in graininess, you can use a 1:1 dilution of this developer.', 'chinese': '为了更好的锐度，但是附带的会多一些颗粒度，可以使用这个显影剂的1：1稀释液。'}\n"
     ]
    }
   ],
   "source": [
    "train_set_path = \"dataset/translation2019zh_train.json\"\n",
    "\n",
    "train_set = []\n",
    "with open(train_set_path) as f:\n",
    "    for line in f:\n",
    "        train_set.append(json.loads(line))\n",
    "\n",
    "print(len(train_set))\n",
    "print(train_set[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to lower the size of this dataset, for testing purposes.\n",
    "- right now 5.1M sentences\n",
    "- let's make it 500,000 sentences..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 'Exhaustion combined with overindulgence could result in minor health problems.', 'chinese': '过度放纵和疲劳会带来身体微恙。'}\n"
     ]
    }
   ],
   "source": [
    "# get 10,000 random indices \n",
    "sampled_indices = np.random.choice(len(train_set), 500000)\n",
    "\n",
    "train_subset = [train_set[i] for i in sampled_indices]\n",
    "print(train_subset[0])\n",
    "with open('dataset/train_set_mini.pkl', 'wb') as f:\n",
    "    pickle.dump(train_subset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'english': 'Exhaustion combined with overindulgence could result in minor health problems.', 'chinese': '过度放纵和疲劳会带来身体微恙。'}\n"
     ]
    }
   ],
   "source": [
    "with open('dataset/train_set_mini.pkl', 'rb') as f:\n",
    "    train_set_mini = pickle.load(f)\n",
    "\n",
    "print(train_set_mini[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train set made. \n",
    "Now to work on the actual architecture\n",
    "\n",
    "## Preprocessing steps:\n",
    "- we want to maintain vocabulary for english and chinese. \n",
    "    - this is simple, just use a counter() and then limit it to if it appears within 5\n",
    "\n",
    "- In terms of encoding, we want to use sequence input, so a sentence becomes a list [3, 100, 8, 9], where each number corresponds to the index of the word in the dictionary\n",
    "    - we want to do this because LSTMs keep track of the word relationships at the sentence level\n",
    "    - then use nn.Embedding?\n",
    "        - nn.Embedding allows us to create a matrix representing the vocabulary. It allows us to train some nuance into the words, where instead of typical BoW where each word is just an index, each word is now a vector, which allows us to train some meaning into the word\n",
    "        - the embedding is a matrix size (vocab length, dim). Length vocab length because each row in the matrix corresponds to a word in the vocab, ( row of index = index of word in vocab :) )\n",
    "        \n",
    "\n",
    "# now to work on the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper functions \n",
    "def remove_punctuation(text):\n",
    "    '''\n",
    "    Get rid of all punctuation from string text\n",
    "    '''\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def get_words_from_sentence(s):\n",
    "    '''\n",
    "    Gets words from sentence \n",
    "    '''\n",
    "    return s.split(' ')\n",
    "\n",
    "def clean_en_pair(pair):\n",
    "    '''\n",
    "    Cleans the english from the pair \n",
    "    '''\n",
    "    return get_words_from_sentence(remove_punctuation(pair['english']).lower())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words pre-clean 9586368\n",
      "Words post-clean 9537657\n",
      "57280\n",
      "16402788\n",
      "6403\n"
     ]
    }
   ],
   "source": [
    "def get_en_vocab(train_set):\n",
    "    '''\n",
    "    get_en_dict:\n",
    "        Gets an english vocab from train_set as a dict \n",
    "    '''\n",
    "    # get only the english sentences, list of strings \n",
    "    en_sentences = [clean_en_pair(pair) for pair in train_set]\n",
    "    en_sentences_flattened = [word for sentence in en_sentences for word in sentence]\n",
    "    print(f\"Words pre-clean {len(en_sentences_flattened)}\")\n",
    "    en_sentences_flattened = [word for word in en_sentences_flattened if word != '']\n",
    "    print(f\"Words post-clean {len(en_sentences_flattened)}\")\n",
    "    \n",
    "    word_counts = Counter(en_sentences_flattened)\n",
    "    # with word counts, now we limit the vocabulary to words that happen at least 5 times\n",
    "    en_vocab = {}\n",
    "    # {word: index}\n",
    "    idx = 0\n",
    "    for word in [\"<SOS>\", \"<EOS>\", \"<UNK>\"]:\n",
    "        en_vocab[word] = idx \n",
    "        idx += 1\n",
    "    for word, occurrences in word_counts.items():\n",
    "        if occurrences >= 5:\n",
    "            en_vocab[word] = idx \n",
    "            idx += 1\n",
    "    return en_vocab\n",
    "\n",
    "def remove_zh_punctuation(text):\n",
    "    cleaned = re.sub(r'[，。！？【】（）《》“”‘’、]', '', text)\n",
    "    cleaned = re.sub(r'\\s+', '', cleaned)\n",
    "    return cleaned\n",
    "\n",
    "def get_zh_vocab(train_set):\n",
    "    '''\n",
    "    get_zh_vocab:\n",
    "        Gets an zh vocab from train_set as a dict \n",
    "    '''\n",
    "    zh_sentences = [list(remove_zh_punctuation(pair['chinese'])) for pair in train_set]\n",
    "    zh_sentences_flattened = [word for sentence in zh_sentences for word in sentence]\n",
    "    print(len(zh_sentences_flattened))\n",
    "\n",
    "    word_counts = Counter(zh_sentences_flattened)\n",
    "    zh_vocab = {}\n",
    "\n",
    "    idx = 0 \n",
    "    for word in [\"<SOS>\", \"<EOS>\", \"<UNK>\"]:\n",
    "        zh_vocab[word] = idx \n",
    "        idx += 1 \n",
    "    for word, occurrences in word_counts.items():\n",
    "        if occurrences >= 2: \n",
    "            zh_vocab[word] = idx \n",
    "            idx += 1 \n",
    "    return zh_vocab\n",
    "\n",
    "en_vocab = get_en_vocab(train_set_mini)\n",
    "print(len(en_vocab))\n",
    "\n",
    "zh_vocab = get_zh_vocab(train_set_mini)\n",
    "print(len(zh_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab/en_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(en_vocab, f)\n",
    "\n",
    "with open('vocab/zh_vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(zh_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab/en_vocab.pkl', 'rb') as f:\n",
    "    en_vocab = pickle.load(f)\n",
    "\n",
    "with open('vocab/zh_vocab.pkl', 'rb') as f:\n",
    "    zh_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model architecture building\n",
    "- 2 LSTM's are the backbone\n",
    "- also build a higher level Seq2Seq model as abstraction of the entire model \n",
    "- nn.Embedding() as a variable for both Encoder and Decoder \n",
    "    - use vocab_size as row length, by the embedding dim as the column length\n",
    "- Encoder will be english, decoder will be chinese "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.LSTM\n",
    "\n",
    "- sequence models are central to NLPl they are models where there is some sort of dependence through teime between inputs. \n",
    "- a recurrent neural network is a network that maintains some kind of state.\n",
    "- for example its output could be used as part of the next input, so that information c an propagate along as the network passes over the sequence.\n",
    "- In the case of an LSTM, for each element in the sequence, there is a corresponding hidden state ht, which in principle contains infromation from arbitrary points earlier in the sequence. \n",
    "- we can use the hidden state to predict words in a language model, pos, and a myriad of oether things.\n",
    "\n",
    "LSTMs in pytorch:\n",
    "- pytorch LSTM expects all of its inputs to be 3D tensors. The semantics of the axes of these tensors is important. The first axis is the sequence itself, the second indixes instances in the mini batch, and the third indexes elements of the input. \n",
    "- ignore mini batching, we will always just have 1 dimension on th second axis.\n",
    "- If we want to run the sequence model over the sentence \"The cow jumped\" our input should look like:\n",
    "\n",
    "[\n",
    "    q (the)\n",
    "    q (cow)\n",
    "    q (jumped)\n",
    "]\n",
    "Except remember there is an additional 2nd dimension with size 1, (this dimension )\n",
    "\n",
    "Initializing an LSTM:\n",
    "```python\n",
    "lstm = nn.LSTM(3, 3) #input dim is 3, output dim is 3 \n",
    "```\n",
    "\n",
    "input_size = 3: this means each input vector at a time step is of length 3. All inputs must have 3 columns, (n x 3). \n",
    "- each sequence = a list of input vectors (one per timestemp)\n",
    "- each input vector = size input_size \n",
    "\n",
    "input_size = 3, then your input tensor shape for 1 batch would be:\n",
    "    (seq_len, batch_size, 3)\n",
    "\n",
    "- What does this mean for the embedding layer?\n",
    "    - it must also be dimension (vocab_size, 3), since each token is mapped to a vecotr using the embedding. This embedding becomes the input at each timestep for the LSTM, and the LSTM accepts vector of dimensions (3).\n",
    "\n",
    "- using nn.LSTM example:\n",
    "    \n",
    "```python\n",
    "self.lstm = nn.LSTM(embedding_dim, hidden_dim)  #(embedding_dim) is the dimension of the embedding dim, and then hidden_dim is also essentially a hyperparameter, it's the dimension of the hidden state \n",
    "```\n",
    "\n",
    "**For most LSTM applications we will need a linear layer to learn the mapping from hidden state to the tag space. but in the case of the encoder, you don't need it! Since the linear layer is essentially the classifier layer, that learns to \n",
    "interpret the hidden layer.**\n",
    "\n",
    "\n",
    "### forward:\n",
    "- in forward we will have\n",
    "```\n",
    "lstm_out, _ = self.LSTM(embeds.view(len(sentence), 1, -1))\n",
    "```\n",
    "- what exactly is going on here? Well, basically embeds is a tensor of dimensions (number of tokens in sentence, embedding dim) (**since rememember that the embedding layer takes each word index, and pulls up the corresponding row from the vocabulary)\n",
    "- we want to reshape it to (sequence length, batch_size, input_size)\n",
    "    - reshaping is: sequence length( the length of the sentence )\n",
    "    - batch size is 1, since its one sentence at a time \n",
    "    - input_size = -1: it assumes that the length will be embedding_dim \n",
    "- **embeds.view**: is a way to reshape in LSTM\n",
    "\n",
    "- forward in the encoder should only return the hidden state and cell state, since its what matters \n",
    "\n",
    "\n",
    "## backpropagation\n",
    "- pytorch under the hood tracks operations on tensors with require_grad = True. All nn.modules like nn.Linear and nn.LSTM already register their parameters with requires_grad=True, so as long as its connected correctly in the forward pass, pytorch will handle the gradients during backprop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder: English layer \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim) # initialize an LSTM, with embedding_dim, and hidden_dim hyperparameters \n",
    "    \n",
    "    def forward(self, sentence):\n",
    "        embeds = self.embeddings(sentence)  # remember that sentence has to the in [word_index0, word_index1, word_index2] form\n",
    "        out , (h_n, c_n) = self.LSTM(embeds.view(len(sentence), 1, -1)) # one timestep at a tiem \n",
    "        return out, (h_n, c_n)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78, 293, 6631]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test example pass through the encoder \n",
    "encoder = Encoder(embedding_dim=3, vocab_size= len(en_vocab), hidden_dim=5)\n",
    "# now remember that for forward we pass a sentence as the list of words mapped to the indices they show up in the vocab, such as [45, 18, 28]\n",
    "sentence = \"I love bread.\"\n",
    "\n",
    "input_words = get_words_from_sentence(remove_punctuation(sentence).lower())\n",
    "# now map the inputs to the vocab \n",
    "input_indices = [en_vocab[word] for word in input_words] \n",
    "# now that I think about it, we probably want a function that does this, so that we don't get hit with a KeyError and actually use our <unk> token lul\n",
    "input_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4061,  0.0178,  0.2490,  0.0442, -0.0523]]],\n",
       "        grad_fn=<StackBackward0>),\n",
       " tensor([[[-0.7400,  0.0740,  0.5608,  0.1064, -0.1235]]],\n",
       "        grad_fn=<StackBackward0>))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with the input_indices, we can now throw it through the encoder?\n",
    "input_indices_tensor = torch.tensor(input_indices, dtype=torch.long)\n",
    "output, hidden = encoder.forward(input_indices_tensor)\n",
    "hidden # this makes sense, we have both hidden and cell states :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n",
    "- so the decoder is another LSTM, taking as input \"a large fixed-dimensional vector representation\", and then use another LSTM to extract the output sequence from that vector \n",
    "- we can just pass in h_n and c_n in the decoder LSTM as parameter for this! \n",
    "- and then for forward we just run the linear layer and then run the log_softmax to get the logits?\n",
    "\n",
    "- What about the Embedding layer?\n",
    "    - we also need an embedding layer ( used in both training and inference )\n",
    "    - training phase we have \"ground-truth\" tokens, we need the \"ground-truth\" tokens we need the embedding layer to make them tensors and to feed each token through the decoder\n",
    "\n",
    "### Teacher forcing and backpropagation\n",
    "- At a time step t, the input is the actual target sequence from t - 1 !. This makes sense, we give it the \"correct\" input from the time before, and have it try to predict the input now.  the t-1 token is called the \"ground truth\" token, is passed through the embedding layer specifically trained for the target language vocabulary\n",
    "- the output is the predicted timestep t token, and what you use to compare as loss is the actual t token. \n",
    "\n",
    "**forward step in the decoder**\n",
    "- when making a prediction, you either use the correct previous token (teacher forcing during training), or you use the previous prediction (inference)\n",
    "- during prediction we will use nn.LSTM. You want to pass the token at t-1's tensor, AND the previous c_n and h_n from the decoder! it's the recurrent aspect of the RNN\n",
    "\n",
    "- we also do feed in the encoder h_n, and c_n, in the first step :).\n",
    "- don't need to worry about the rest, because it already does it implicitly by nn.LSTM\n",
    "\n",
    "- also to enforce teacher forcing, we have to do one time step at a time, instead of all at once \n",
    "    - so instead, we go through every single input one at time \n",
    "\n",
    "- in the no teacher YOU USE THE PREVIOUS PREDICTION OF THE TIME(after it is )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurations \n",
    "MAX_RESPONSE_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Luong attention general form (with Linear layer)\n",
    "# unsure what the shape should be at the moment \n",
    "# make just a linear transformation with no bias\n",
    "embedding_dim = 32 \n",
    "hidden_dim = 128 \n",
    "\n",
    "general_attn_layer = nn.Linear(in_features=hidden_dim, out_features=hidden_dim, bias=False)\n",
    "\n",
    "class LuongAttnDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_dim, device):\n",
    "        super(LuongAttnDecoder, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim * 2, vocab_size) # 2x hidden dim now for result after concat attention input\n",
    "        self.device = device \n",
    "    \n",
    "    def word_to_tensor(self, word):\n",
    "        '''\n",
    "        takes a single wrod and gets the corresponding tensor\n",
    "        '''\n",
    "        word_lst = get_words_from_sentence(remove_zh_punctuation(word))\n",
    "        indices = [zh_vocab[word] for word in word_lst]\n",
    "        # get tensor \n",
    "        return torch.tensor(indices, dtype=torch.long).to(self.device)\n",
    "\n",
    "    def forward(self, hidden, encoder_out, sentence=None):\n",
    "        '''\n",
    "        does the forward propagation. If sentence is provided, then we do teacher-forcing. Else we assume it is inference  \n",
    "            Params:\n",
    "                hidden: the hidden state passed from the previous \n",
    "                sentence: a sentence to be used for teacher-forcing, as a tensor \n",
    "                Make sure the teacher-forcing sentence is sliced to not include the last token [:-1]\n",
    "        '''\n",
    "        # teacher-forcing training, we are going to \"undo\" vectorization]\n",
    "        all_outputs = []\n",
    "        if sentence is not None:\n",
    "            embeds_tensor = self.embeddings(sentence)\n",
    "            for word_tensor in embeds_tensor:\n",
    "                out, hidden = self.LSTM(word_tensor.view(1, 1, -1), hidden)\n",
    "                # pass encoder out to attention layer \n",
    "                attn_scores = general_attn_layer(encoder_out) @ hidden[0].squeeze()\n",
    "                # now with attn scores, we want to softmax the scores \n",
    "                softmaxed_scores = torch.nn.functional.softmax(attn_scores, dim=0)\n",
    "                # multiply by encoder_out\n",
    "                # now that they are softmaxed, we want to multiply by all encoder states to give a weighted tensor, we can broadcast it as well \n",
    "                weighted_encoder_hidden_states = softmaxed_scores * encoder_out.squeeze()\n",
    "                # sum the tensor \n",
    "                context = torch.sum(weighted_encoder_hidden_states, dim=0).view(1, 1, -1)\n",
    "                # concat the context vector with the hidden state \n",
    "                combined_tensor = torch.concat([context, hidden[0]], dim=-1)\n",
    "                logits = self.linear(combined_tensor)\n",
    "                all_outputs.append(logits)\n",
    "        else:\n",
    "            start_token = self.word_to_tensor('<SOS>')\n",
    "            # run through embedding layer\n",
    "            prev_char = start_token\n",
    "            for i in range(MAX_RESPONSE_LENGTH):\n",
    "                if prev_char.item() == 1:\n",
    "                    break\n",
    "                embeds = self.embeddings(prev_char).to(self.device)\n",
    "                out, hidden = self.LSTM(embeds.view(1, 1, -1), hidden)\n",
    "                attn_scores = general_attn_layer(encoder_out) @ hidden[0].squeeze()\n",
    "                softmaxed_scores = torch.nn.functional.softmax(attn_scores, dim=0)\n",
    "                weighted_encoder_hidden_states = softmaxed_scores * encoder_out.squeeze()\n",
    "                context = torch.sum(weighted_encoder_hidden_states, dim=0).view(1, 1, -1)\n",
    "                combined_tensor = torch.concat([context, hidden[0]], dim=-1)\n",
    "                logits = self.linear(combined_tensor)\n",
    "                all_outputs.append(logits)\n",
    "                pred_idx = torch.argmax(logits, dim=2).item()\n",
    "                prev_char = torch.tensor(pred_idx, dtype=torch.long, device=self.device)\n",
    "        return torch.cat(all_outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions to take a sentence and turn it into a tensor, adding <sos> and <eos>\n",
    "def sequence_to_tensor_en(sequence):\n",
    "    '''\n",
    "    takes sequence and converts to tensor \n",
    "    '''\n",
    "    # add \"<SOS> and <EOS>\"\n",
    "    words = get_words_from_sentence(\"<SOS> \" + remove_punctuation(sequence).lower() + \" <EOS>\")\n",
    "    \n",
    "    # convert to indices, reverting to <UNK> token\n",
    "    word_indices = [ en_vocab[word] if word in en_vocab else en_vocab[\"<UNK>\"] for word in words ]\n",
    "    return torch.tensor(word_indices, dtype=torch.long)\n",
    "    \n",
    "\n",
    "def sequence_to_tensor_zh(sequence):\n",
    "    '''\n",
    "    takes sequence and converts to chinese tensor \n",
    "    '''\n",
    "    words = ([\"<SOS>\"] + list(remove_zh_punctuation(sequence)))\n",
    "    words.append(\"<EOS>\")\n",
    "    \n",
    "    word_indices = [ zh_vocab[word] if word in zh_vocab else zh_vocab[\"<UNK>\"] for word in words ]\n",
    "    return torch.tensor(word_indices, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train  500000\n",
      "Length of zh dictionary  6403\n",
      "Length of english dictionary  57280\n"
     ]
    }
   ],
   "source": [
    "## new 100,000 len\n",
    "print(\"Length of train \", len(train_set_mini))\n",
    "print(\"Length of zh dictionary \", len(zh_vocab))\n",
    "print(\"Length of english dictionary \", len(en_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zh_tensor_outputs_to_sentence(output_tensor):\n",
    "    s = ''\n",
    "    zh_vocab_lst = list(zh_vocab.keys())\n",
    "    for word_tensor in output_tensor:\n",
    "        pred_idx = torch.argmax(word_tensor, dim=-1).item()\n",
    "        s += zh_vocab_lst[pred_idx]\n",
    "    return s "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does mps save time?\n",
    "琥ä纷随哗良钮眼黎s\n",
    "Number of trains 1000\n",
    "Loss 6.893100890159607\n",
    "time for 1000 : mps: 13.119836807250977\n",
    "\n",
    "CPU:\n",
    "Number of trains 1000\n",
    "Loss 6.896958403587341\n",
    "time for 1000 : cpu: 23.142292022705078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "蚀蚊哖俯鼐哖柒盈答柒\n",
      "Number of trains 10000\n",
      "Loss 5.936476695370674\n",
      "Number of trains 20000\n",
      "Loss 5.345361726331711\n",
      "Number of trains 30000\n",
      "Loss 5.1352989352464675\n",
      "Number of trains 40000\n",
      "Loss 5.0086994344949725\n",
      "Number of trains 50000\n",
      "Loss 4.925433911287785\n",
      "Number of trains 60000\n",
      "Loss 4.8515591985464095\n",
      "Number of trains 70000\n",
      "Loss 4.802882876157761\n",
      "Number of trains 80000\n",
      "Loss 4.757068572258949\n",
      "Number of trains 90000\n",
      "Loss 4.702026947426796\n",
      "Number of trains 100000\n",
      "Loss 4.675128614723683\n",
      "Number of trains 110000\n",
      "Loss 4.646908678662777\n",
      "Number of trains 120000\n",
      "Loss 4.620252806901932\n",
      "Number of trains 130000\n",
      "Loss 4.594933414840698\n",
      "Number of trains 140000\n",
      "Loss 4.578614836573601\n",
      "Number of trains 150000\n",
      "Loss 4.549867865610123\n",
      "Number of trains 160000\n",
      "Loss 4.534059087669849\n",
      "Number of trains 170000\n",
      "Loss 4.530960413765907\n",
      "Number of trains 180000\n",
      "Loss 4.508411643350124\n",
      "Number of trains 190000\n",
      "Loss 4.483580934113264\n",
      "Number of trains 200000\n",
      "Loss 4.4742356302976605\n",
      "Number of trains 210000\n",
      "Loss 4.457356423145533\n",
      "Number of trains 220000\n",
      "Loss 4.454313436752558\n",
      "Number of trains 230000\n",
      "Loss 4.458455623996258\n",
      "Number of trains 240000\n",
      "Loss 4.426148200821877\n",
      "Number of trains 250000\n",
      "Loss 4.43009796050787\n",
      "Number of trains 260000\n",
      "Loss 4.398555866885185\n",
      "Number of trains 270000\n",
      "Loss 4.404176840507984\n",
      "Number of trains 280000\n",
      "Loss 4.384858815789222\n",
      "Number of trains 290000\n",
      "Loss 4.405791341364384\n",
      "Number of trains 300000\n",
      "Loss 4.369736851680279\n",
      "Number of trains 310000\n",
      "Loss 4.363559592998028\n",
      "Number of trains 320000\n",
      "Loss 4.364001580619812\n",
      "Number of trains 330000\n",
      "Loss 4.369051623898745\n",
      "Number of trains 340000\n",
      "Loss 4.34892216681242\n",
      "Number of trains 350000\n",
      "Loss 4.347393543481827\n",
      "Number of trains 360000\n",
      "Loss 4.349856234014034\n",
      "Number of trains 370000\n",
      "Loss 4.34249507265687\n",
      "Number of trains 380000\n",
      "Loss 4.322560148298741\n",
      "Number of trains 390000\n",
      "Loss 4.31163405367434\n",
      "Number of trains 400000\n",
      "Loss 4.30037235301137\n",
      "Number of trains 410000\n",
      "Loss 4.326145485758781\n",
      "Number of trains 420000\n",
      "Loss 4.313319571167231\n",
      "Number of trains 430000\n",
      "Loss 4.309350853645801\n",
      "Number of trains 440000\n",
      "Loss 4.294604516267777\n",
      "Number of trains 450000\n",
      "Loss 4.301159613478184\n",
      "Number of trains 460000\n",
      "Loss 4.278116769182682\n",
      "Number of trains 470000\n",
      "Loss 4.275945482504368\n",
      "Number of trains 480000\n",
      "Loss 4.3001302114486695\n",
      "Number of trains 490000\n",
      "Loss 4.302107378768921\n",
      "Number of trains 500000\n",
      "Loss 4.2756709194839\n",
      "我爱我爱<EOS>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m decoder_trained\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     54\u001b[0m general_attn_layer\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set_mini\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_trained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_trained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 38\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, training_data, encoder, decoder, device, lr)\u001b[0m\n\u001b[1;32m     36\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 38\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     42\u001b[0m out, predict_hidden \u001b[38;5;241m=\u001b[39m encoder(predict_en_tensor)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def train(num_epochs, training_data, encoder, decoder, device, lr=0.001):\n",
    "    optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()), lr=lr\n",
    ")\n",
    "    count = 0\n",
    "    total_loss = 0\n",
    "    # see random prediction\n",
    "    predict_en_sequence = \"I love bread\" \n",
    "    predict_en_tensor = sequence_to_tensor_en(predict_en_sequence).to(device)\n",
    "    out, predict_hidden = encoder(predict_en_tensor)\n",
    "    out = out.to(device)\n",
    "    print(zh_tensor_outputs_to_sentence(decoder.forward(predict_hidden, encoder_out=out)))\n",
    "    start_time = time.time()\n",
    "    for i in range(num_epochs):\n",
    "        for pair in training_data:\n",
    "            count += 1\n",
    "            if count % 10000 == 0:\n",
    "                print(f\"Number of trains {count}\")\n",
    "                # print the loss \n",
    "                print(f\"Loss {total_loss / 10000}\")\n",
    "                total_loss = 0\n",
    "            english = pair['english']\n",
    "            zh = pair['chinese']\n",
    "            en_tensor = sequence_to_tensor_en(english)\n",
    "            zh_tensor = sequence_to_tensor_zh(zh)\n",
    "            # pass to device \n",
    "            en_tensor = sequence_to_tensor_en(english).to(device)\n",
    "            zh_tensor = sequence_to_tensor_zh(zh).to(device)\n",
    "\n",
    "            out, hidden = encoder.forward(en_tensor)\n",
    "            target = zh_tensor[1:]\n",
    "            predicted = decoder.forward(hidden, encoder_out=out, sentence = zh_tensor[:-1])\n",
    "            loss = nn.functional.cross_entropy(torch.squeeze(predicted), target)\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        \n",
    "        out, predict_hidden = encoder(predict_en_tensor)\n",
    "        out = out.to(device)\n",
    "        print(zh_tensor_outputs_to_sentence(decoder.forward(predict_hidden, out)))\n",
    "    print(f\"Total training time: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "encoder_trained = Encoder(embedding_dim=32, vocab_size=len(en_vocab), hidden_dim=128)\n",
    "decoder_trained = LuongAttnDecoder(embedding_dim=32, vocab_size=len(zh_vocab), hidden_dim=128, device=device)\n",
    "print(device)\n",
    "encoder_trained.to(device)\n",
    "decoder_trained.to(device)\n",
    "general_attn_layer.to(device)\n",
    "train(4, train_set_mini, encoder_trained, decoder_trained, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save encoder and decoder \n",
    "torch.save(encoder_trained.state_dict(), './trained_models/baseline_encoder.pth')\n",
    "torch.save(decoder_trained.state_dict(), './trained_models/baseline_decoder.pth')\n",
    "torch.save(general_attn_layer.state_dict(), './trained_models/baseline_attn_layer.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTENTION MECHANISM\n",
    "- we will now work on adding the attention mechanism\n",
    "- we will try to add Luong attention \n",
    "- current reading source:   https://arxiv.org/pdf/1508.04025\n",
    "\n",
    "### Luong attention in the model\n",
    "- modfies the decoder\n",
    "- at each decoding step, instead of relying only on the previous hidden state, the decoder now looks back at all encoder outputs and chooses which parts to pay attention to. \n",
    "\n",
    "Key ideas:\n",
    "    - Encoder still processes the whole input and outputs as sequence of hidden states:\n",
    "\n",
    "    - Decoder at time step t has its own hidden state st.\n",
    "    - Attention score is computed between st (current decoder hidden state) and each hi (encoder hidden state)\n",
    "    - scoring function (how you actually calculate the attention score )\n",
    "        - dot product score(st, hi) = st * Wa * hi \n",
    "    \n",
    "    - for all encoder outputs, softmax the scores to get a probability distribution over input positions  (adds up to 1)\n",
    "\n",
    "    - context vector:\n",
    "        - weighted sum of encoder hidden states using the attention weights \n",
    "        - a \"summary\" of the input that the decoder should focus on at step t \n",
    "    \n",
    "    final step: combine the context vector, and the decoder hidden state to predict the next word. \n",
    "\n",
    "- decoder hidden --> compute scores with all encoder outputs --> softmax --> get context --> combine with decoder hidden --> predict word \n",
    "\n",
    "\n",
    "### score\n",
    "- we calculate a score vector\n",
    "- this score vector is softmaxed, this turns it into a probability vector (adds up to 1)\n",
    "- whatever we multiply with this softmaxed score vector has the effect of being amplified or ignored. If the corresponding attention cell is close to 1, it is amplified, but if it is small, the value goes to 0\n",
    "\n",
    "- we are going to use Luong Attention general version \n",
    "\n",
    "$$\n",
    "score = h_t^{T} W_a h_s\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preimplementation thoughts\n",
    "\n",
    "1. Encoder stays the same. we just keep track of out, as it represents the entire hidden state across all time steps.\n",
    "\n",
    "out, hidden = Encoder(sequence)\n",
    "\n",
    "2. We change the teacher forcing code to be one time step at a time (not vectorized, going to map it exactly as the inference logic. \n",
    "\n",
    "I will apply below to both the teacher forcing and inference blocks:\n",
    "\n",
    "3. run:\n",
    "out, hidden = self.LSTM(embeds.view(1, 1, -1), hidden)\n",
    "\n",
    "to give us the hidden state for this current time step.\n",
    "\n",
    "3. With encoder out, representing all hidden states, and hidden representing the decoder hidden state for this current time step, we have all we need to create the SCORES tensor.\n",
    "\ta. We do this by passing in encoder OUT to a NEW linear layer (I still need to figure out the dimensions, but I'll just do some math during the implementation time). This is dot with decoder hidden . I know it should be out, because we want to use the ALL hidden states, that's the point of attention. \n",
    "        - remember for dot, this is matrix multiplication , so inner dimensions must match\n",
    "        - the inner dimennsions for encoder out and hidden for decoder must match\n",
    "        - encoder_out is [seq_len, 1, hidden_dim]\n",
    "        - decoder hidden is [1, 1, hidden_dim]\n",
    "        - so in other words, our output must maintain hidden_dim, so its a square matrix? lol \n",
    "\n",
    "    b. Now we will have a scores tensor. I softmax this so as to create a vector that can amplify / minimize whatever its multiplied by. I multiply this softmax vector with out (all encoder hidden states). This gives a weighted out tensor, I sum it up to give us a single context tensor.\n",
    "        \n",
    "        - since our goal is to amplify each hidden state or deamplify, based on the corresponding attention value, we want elementwise multiplication. Since they are the size of embedding dim, it should be element row multiplication. Expand the softmazed scores to match the hidden states shape (using broadcasting.)\n",
    "\n",
    "\n",
    "4. with the context tensor, we concatenate with CURRENT decoder hidden state torch.cat(context, hidden)\n",
    "\n",
    "5. We pass this combined tensor to the already existing linear layer. By already existing linear layer, i mean the second line of this code where we get the logits\n",
    "\n",
    "                out, hidden = self.LSTM(embeds.view(1, 1, -1), hidden)\n",
    "                logits = self.linear(out) # this will be self.linear(combined_tensor)\n",
    "    **IMPORTANT** we now need to adjust the linear layer to be 2xhidden_dim, since our time step input tensor to the linear layer is now essentially the combinnation of 2 tensors \n",
    "6. everything else stays the same\n",
    "\n",
    "\n",
    "At step 3, we need to make sure to get rid of the batch dimension \n",
    "    - use squeeze to get rid of the batch dimenision, otherwise it won't line up even with a transpose "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training runs\n",
    "1. 04/25/25: first successful training run, 2 epochs @ 100k examples each. Model outputs non-garbage. Recognizes pronouns, and uses EOS tag. \n",
    "2. 04/25/25: Added vectorization to teacher forcing forward code. Use MPS, 3 epochs @200k examples each. Loss drops to around 4.6. Model still learns pronouns, but also common words like \"need\" --> 想要\n",
    "\n",
    "3. 04/27/25: . 4 epochs at 500,000 pairs. Added Luong attention\n",
    "- had to stop after one epoch because it was taking too long, but loss dropped to 4.27.  (04/28/25)\n",
    "- \"I love bread\" --> \"我爱我爱<EOS>\" (Got I love)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
