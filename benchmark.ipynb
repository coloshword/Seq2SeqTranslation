{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd4de2c",
   "metadata": {},
   "source": [
    "## Benchmarking nn.Linear vs nd.Linear in a Sequence To Sequence Machine Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa08f4e",
   "metadata": {},
   "source": [
    "### Overview\n",
    "**this is my application to Ensemble AI's ML research intern & ML engineering intern positions** \n",
    "\n",
    "- We will be implementing a Sequence To Sequence model for English to Chinese machine translation. \n",
    "- The attention mechanism we will be implementing will be Luong Attention, general form, as it makes use of another Linear layer. \n",
    "- We will run 2 experiments benchmarking nn.Linear vs nd.Linear:\n",
    "\n",
    "1. **Performance Benchmarking**\n",
    "We will compare final performance between the Seq2Seq models using nn.Linear and nd.Linear, keeping model size the same. \n",
    "\n",
    "2. **Parameter size analysis**\n",
    "We will evaluate whether a Seq2Seq model using nd.Linear will perform similarly to the same model architecture using nn.Linear, but with a lower parameter count; specifically, the hidden dim will be 256 in the nn.Linear implementation, vs 192 in the nd.Linear implementation, representing a ~25% reduction in parameter count."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ae7b7a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
